[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MSBA R Bootcamp",
    "section": "",
    "text": "1 Installing R and R Studio",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#take-your-first-look-at-r",
    "href": "index.html#take-your-first-look-at-r",
    "title": "MSBA R Bootcamp",
    "section": "1.1 Take Your First Look at R",
    "text": "1.1 Take Your First Look at R\n\nOn the right, two windows, each with tabs\n\nThere are many ways to customize the look of RStudio for accessibility purposes with a couple being the following:\n\nYou can resize the windows using the splitters.\nYou can maximize/restore the windows within the left/right panels using the familiar Windows controls in the upper-right of each window.\n\n\nOn the left, the Console Window reproduces the R environment.\n\nObserve the command line with the \\(&gt;\\) symbol.\n\n\n\n\n\nConsole",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#creating-a-project-for-our-bootcamp",
    "href": "index.html#creating-a-project-for-our-bootcamp",
    "title": "MSBA R Bootcamp",
    "section": "1.2 Creating a Project for Our Bootcamp",
    "text": "1.2 Creating a Project for Our Bootcamp\n\nThe RStudio project file is a file that sits in the root directory, with the extension .Rproj. When your RStudio session is running through the project file (.Rproj), the current working directory points to the root folder where that .Rproj file is saved.\nOpen up R Studio and Let’s Take a look around.\nStart by creating a project for our class. Projects are great because they aid in your organization technique.\nYou will find that some professors are not insistent on making a project for their class, but it is helpful to still do to organize your materials. You will have a lot of code in this class, so it is good practice to keep everything organized!\nTo create a project click \\(File &gt; New Project - New Directory &gt; New Project\\) and save your project to a place on your computer (not the cloud).\n\n\n\n\nCreateProject\n\n\n\n\n\nCreating a New Project",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#r-script-files-myfirstrscript.r",
    "href": "index.html#r-script-files-myfirstrscript.r",
    "title": "MSBA R Bootcamp",
    "section": "1.3 R Script files (MyFirstRscript.R)",
    "text": "1.3 R Script files (MyFirstRscript.R)\n\nEntering and running code at the R command line is effective and simple. However, each time you want to execute a set of commands, you have to re-enter them at the command line. Nothing saves for later.\nComplex commands are particularly difficult causing you to re-entering the code to fix any errors typographical or otherwise. Fortunately, you can make a .R script file to solve this issue.\nA .R script is simply a text file containing a set of commands and comments. The script can be saved and used later to rerun the code. The script can also be documented with comments and edited again and again to suit your needs.\n\nCreate your first R Script file within your Project for testing purposes.\n\nGo to File &gt; New File &gt; R Script\nSave this file as MyFirstRscript.R in your project folder you just made. You should see this new file under Files like mine is in the bottom right panel. As we create new files, continue to save them into your project folder.\n\nOn the .R file presented to the left, comments are added as denoted by the hashtag which you can type or push ctrl + shift + c.\nIf you type in your console, it will not save it for later. However, if you save code in this R Script file, you can open your file at a later date to rerun your code. Also, as we move through the class, feel free to document all your notes in your .R file via # called comments. More on comments later.\n\n\n\n\nScreenshot of MyFirstScript.R",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#using-r-more-effectively",
    "href": "index.html#using-r-more-effectively",
    "title": "MSBA R Bootcamp",
    "section": "1.4 Using R More Effectively",
    "text": "1.4 Using R More Effectively\n\n1.4.1 Setting Options to Your Liking\n\nYou have many options to customize R Studio.\nGo to Tools &gt; Global Options to see the list of options for you to customize.\nA few are listed below.\n\nYou can set your general information including your default working directory (when not in a project).\nYou can customize the appearance to a theme that accommodates your learning style and visual preferences.\nYou can turn on a spell check.\n\n\n\n\n\nTools &gt; Global Options.R\n\n\n\n\n1.4.2 Quick Keys in R\n\nThere are a lot of quick keys in R to make you able to use it faster and more effectively. You may look over these and try on your own.\n\n\n\n\nConsole Quick Keys\n\n\n\n\n\nSource Quick Keys\n\n\n\n\n\nEditing Quick Keys\n\n\n\n\n1.4.3 Getting Help in R\n\nThere are lots of ways to get help in R.\nIn R, use the help search box to find information on a function, parameter, or package.\n\n?mean\nhelp.search(‘swirl’)\nhelp(package = ‘MASS’)\n?install.packages\n\n\n\n\n\nGetting Help\n\n\n\nYou should try to look up the tapply command to see what it does.\n\nUse ?tapply in your .R file to pull up tapply() command or type tapply in the Help box. * Formally, you should see that the command applies a function to each cell of a ragged array, that is to each (non-empty) group of values given by a unique combination of the levels of certain factors. This means that the command does some math calculation (mean, sum, etc.) on a continuous variable after dividing the data by group.\n\nThe format is tapply(x, index, and fun), where x is a continuous variable, index is a grouping variable or factor, and fun is a function like mean.\nMore on that function later in the lesson.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#accessing-data-files-for-the-book",
    "href": "index.html#accessing-data-files-for-the-book",
    "title": "MSBA R Bootcamp",
    "section": "1.5 Accessing Data Files for the Book",
    "text": "1.5 Accessing Data Files for the Book\n\nTo download the data sets, go to our Blackboard datasets page and download them to your computer. This will be available on the first day of bootcamp.\nOnce downloaded, unzip the file by right-clicking and selecting “Extract All”, and then move the subfolder named data to your working directory.\nIn the example below, my project folder for the class is called BUAD502 and the subfolder that contains all the data files is called data. You can put your folder anywhere on the hard drive of your computer, but do not download it to places on the cloud like OneDrive.\n\n\n\n\nSaving Our Book Files Into a “data” Folder in Your Working Directory\n\n\n\nWe will use a number of data files plus more for homework/projects, so be prepared to use these files throughout the class.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "MSBA R Bootcamp",
    "section": "1.6 Summary",
    "text": "1.6 Summary\n\nAt the end of this section, you should have downloaded and browsed R in RStudio. You should have looked at the quick keys in R to make editing your R documents easier. You should set up a project folder for the course where you want to on your computer, and made your first .R script file located in that project folder. You should have tried a line of code or two as presented. Finally, you should have looked around the RStudio environment and found the help tab.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "index.html#creating-a-project",
    "href": "index.html#creating-a-project",
    "title": "MSBA R Bootcamp",
    "section": "1.2 Creating a Project",
    "text": "1.2 Creating a Project\n\nThe RStudio project file is a file that sits in the root directory, with the extension .Rproj. When your RStudio session is running through the project file (.Rproj), the current working directory points to the root folder where that .Rproj file is saved.\nOpen up R Studio and Let’s Take a look around.\nStart by creating a project for our class. Projects are great because they aid in your organization technique.\nYou will find that some professors are not insistent on making a project for their class, but it is helpful to still do to organize your materials. You will have a lot of code in this class, so it is good practice to keep everything organized!\nTo create a project click \\(File &gt; New Project - New Directory &gt; New Project\\) and save your project to a place on your computer (not the cloud).\n\n\n\n\nCreateProject\n\n\n\n\n\nCreating a New Project",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing R and R Studio</span>"
    ]
  },
  {
    "objectID": "introR.html",
    "href": "introR.html",
    "title": "2  Introduction to R and RStudio",
    "section": "",
    "text": "2.0.1 At a Glance",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#what-is-statistics",
    "href": "introR.html#what-is-statistics",
    "title": "2  Introduction to R and RStudio",
    "section": "2.1 What is Statistics?",
    "text": "2.1 What is Statistics?\n\nStatistics is the methodology of extracting useful information from a data set.\nNumerical results are not very useful unless they are accompanied with clearly stated actionable business insights.\nTo do good statistical analysis, you must do the following:\n\nFind the right data.\nUse the appropriate statistical tools.\nClearly communicate the numerical information in written language.\n\nWith knowledge of statistics:\n\nAvoid risk of making uninformed decisions and costly mistakes.\nDifferentiate between sound statistical conclusions and questionable conclusions.\n\nData and analytics capabilities have made a leap forward.\n\nGrowing availability of vast amounts of data.\nImproved computational power.\nDevelopment of sophisticated algorithms.\n\n\n\n2.1.1 Two Main Branches of Statistics\n\nDescriptive Statistics - collecting, organizing, and presenting the data.\nInferential Statistics - drawing conclusions about a population based on sample data from that population.\n\nA population consists of all items of interest.\nA sample is a subset of the population.\nA sample statistic is calculated from the sample data and is used to make inferences about the population parameter.\n\nReasons for sampling from the population:\n\nToo expensive to gather information on the entire population.\nOften impossible to gather information on the entire population.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#setting-up-r",
    "href": "introR.html#setting-up-r",
    "title": "2  Introduction to R and RStudio",
    "section": "2.2 Setting up R",
    "text": "2.2 Setting up R\n\n2.2.1 Why R?\n\nR is a very sophisticated statistical software that allows you to enter commands one-at-a-time, or write scripts using the R language.\nEasily installed, state-of-the-art, and it is free and open source and supported by a well-established R Community.\nR can be used with RStudio, which is a graphical user interface that allows you to do the following:\n\nwrite, edit, and execute code;\ngenerate, view, and store plots;\nmanage files, objects and data frames;\nintegrate with version control systems.\n\nR comes with community that helps in the development of R resources.\n\nA package is developed by R users to do one specific thing or a set of related things that could store a collection of functions, data, and code.\nA library is the place where the package is located on your computer.\n\nA repository is a central location where many developed packages are located and available for download. There are 3 big repositories, but we use Comprehensive R Archive Network, or CRAN, which is R’s main repository with over 18,000 packages available.\nR’s community is vast, and you can always seek information from the community to try to help you with a R related issue.\nR is Called a Dynamically Typed Language\nIn R, a variable itself is not declared of any data type.\nRather it receives the data type of the R object that is assigned to it.\nWe can change a variable’s data type if we want, but it will inherit one based on the object assigned to it. We will learn some common ways to do this in the data preparation section.\n\n\n\n2.2.2 Creating a Project for Our Class\n\nThe RStudio project file is a file that sits in the root directory, with the extension .Rproj. When your RStudio session is running through the project file (.Rproj), the current working directory points to the root folder where that .Rproj file is saved.\n\nR projects are a type of file which function with RStudio.\nThey have the .Rproj file extension.\nR projects are each associated with a directory.\nThey are useful when working with many files for one purpose, hence the name “project”\nA great feature is that they “know” which files are relevant to a project, when you open the project RStudio will load those files automatically.\n\nStart by creating a project for our class. Projects are great because they aid in your organization technique.\nYou will find that some professors are not insistent on making a project for their class, but it is helpful to still do to organize your materials. You will have a lot of code in this program!\nTo create a project click \\(File &gt; New Project - New Directory &gt; New Project\\) and save your project to a place on your computer (not the cloud).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#using-r-script-files",
    "href": "introR.html#using-r-script-files",
    "title": "2  Introduction to R and RStudio",
    "section": "2.3 Using R Script Files",
    "text": "2.3 Using R Script Files\n\nUsing R Script Files:\n\nA .R script is simply a text file containing a set of commands and comments. The script can be saved and used later to rerun the code. The script can also be documented with comments and edited again and again to suit your needs.\n\nUsing the Console\n\nEntering and running code at the R command line is effective and simple. However, each time you want to execute a set of commands, you must re-enter them at the command line. Nothing saves for later.\n\nComplex commands are particularly difficult causing you to re-entering the code to fix any errors typographical or otherwise.R script files help to solve this issue.\n\n\n2.3.1 Create a New R Script File: Chapter1.R\n\nTo save your notes from today’s lecture, create a .R file named Chapter1.R and save it to your project file you made in the last class.\nThere are a couple of parts to this chapter, and we can add code from today’s chapter in one file so that our code is stacked nicely together.\n\nFor each new chapter, start a new file and save it to your project folder.\n\n\n\n\nScreenshot of R Environment\n\n\n\n\n2.3.2 Using a Prolog\n\nWe should include a prolog to introduce each R Script File during the course.\nA prolog is a set of comments at the top of a code file that provides information about what is in the file.\nIncluding a prolog is considered coding best practice.\nIt also names the files and resources used that facilitates identification.\nAn informal prolog is below:\n\n\n####################################\n# Project name: \n# Project purpose: \n# Code author name: \n# Date last edited: \n# Data used: \n# Libraries used: \n####################################\n\n\nOn your R Script File, add your own prolog following the template as shown.\nI like to add a line for Data used and Libraries used so we know what all we used in the script.\n\n\n####################################\n# Project name: Chapter 1\n# Project purpose: To create an R script file to learn about R. \n# Code author name: Pamela Schlosser\n# Date last edited: [Enter Date Here]\n# Data used: NA\n# Libraries used: NA\n####################################\n\n\nThen, as we work through our .R script and add data files or libraries to our code, we go back and edit the prolog.\n\n\n\n2.3.3 R Handles Text in Multiple Ways\n\nR can generally use single quotes or double quotes when marking text. However, if you use a single quote to start, use a single quote to end. The same for double quotes - ensure the pairing is the same quote type.\nYou sometimes need to be careful with nested quotes, but generally it does not matter which you use.\n\n\n\"This is a string\"\n\n[1] \"This is a string\"\n\n\"This is also a string\"\n\n[1] \"This is also a string\"\n\n\n\nWe can also add comments to our code to document our work and add notes to our self or to others.\n\n\n# This is a comment for documentation or annotation\n\n\nAdd the code above to your R file and run each line using Ctl + Enter or select all lines and click Run.\nTake note that nothing prints in the console after running a comment.\n\n\n\n\nChapter 1 R File\n\n\n\n\n2.3.4 Using Comments\n\nWe use comments to organize and explain code in our R Script file.\nBe sure to write clear code that does not need a lot of comments.\nInclude useful comments where needed so that anyone (including yourself in the future) can run and understand your code.\nIf something does not work, don’t delete it yet. Instead, comment it out while you troubleshoot it or to try different alternatives.\nNotice the prolog above is in comments.\n\n\n\n2.3.5 Note on R Markdown\n\nThese files were formatted with RMarkdown. RMarkdown is a simple formatting syntax for authoring documents of a variety of types, including PowerPoint and html files.\nOn the document, RMarkdown prints the command and then follows the command with the output after 2 hashtags.\nIn your R Script File, you only need to type in the command and then run your code to get the same output as presented here.\n\n\n\n\nReading our HTML file\n\n\n\n\n2.3.6 R is an Interactive Calculator\n\nAn important facet of R is that it should serve as your sole calculator.\nTry these commands in your .R file by typing them in and clicking Ctr + Enter on each line.\n\n\n3 + 4\n\n[1] 7\n\n3 * 4\n\n[1] 12\n\n3/4\n\n[1] 0.75\n\n3 + 4 * 100^2\n\n[1] 40003\n\n\n\nTake note that order of operations holds in R: PEMDAS\n\nParentheses ()\nExponents ^ and \\(**\\)\nDivision \\(/\\), Multiplication \\(*\\), modulo, and integer division\nAddition + and Subtraction -\n\nNote that modulo and integer division have the same priority level as multiplication and division, where modulo is just the remainder.\n\n\nprint(2 + 3 * 5 - 7^2%%4 + (5/2))\n\n[1] 18.5\n\n5/2  #parentheses: = 2.5\n\n[1] 2.5\n\n7^2  #exponent:= 49\n\n[1] 49\n\n3 * 5  #multiplication: = 15\n\n[1] 15\n\n17%%4  #modulo: = 1\n\n[1] 1\n\n17%/%4  #integer division: = 4\n\n[1] 4\n\n2 + 15  #addition: = 17\n\n[1] 17\n\n17 - 1  #subtraction: = 16\n\n[1] 16\n\n16 + 2.5  #addition: = 18.5\n\n[1] 18.5\n\n\n\n\n2.3.7 Running Commands\n\nThere are a few ways to run commands via your .R file.\n\nYou can click Ctr + Enter on each line.\nYou can select all the lines you want to run and select Ctr + Enter.\nYou can select all the lines you want to run and select the run button as shown in the Figure.\n\n\n\n\n\nRun Code\n\n\n\nNow that I have asked you to add a couple lines of code, after this point, when R code is shown on this file, you should add it to your .R script file along with any notes you want. I won’t explicitly say - “add this code.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#observations-and-variables",
    "href": "introR.html#observations-and-variables",
    "title": "2  Introduction to R and RStudio",
    "section": "2.4 Observations and Variables",
    "text": "2.4 Observations and Variables\n\nGoing back to the basics in statistics, we need to define an observation and variable so that we can know how to use them effectively in R in creating objects.\nAn Observation is a single row of data in a data frame that usually represents one person or other entity.\nA Variable is a measured characteristic of some entity (e.g., income, years of education, sex, height, blood pressure, smoking status, etc.).\nIn data frames in R, the columns are variables that contain information about the observations (rows).\n\nNote that we will break this code down later.\n\n\nincome &lt;- c(34000, 123000, 215000)\nvoted &lt;- c(\"yes\", \"no\", \"no\")\nvote &lt;- data.frame(income, voted)\nvote\n\n  income voted\n1  34000   yes\n2 123000    no\n3 215000    no\n\n\nObservations: People being measured.\nVariables: Information about each person (income and voted).\n\n\n# Shows the number of columns or variables\nncol(vote)\n\n[1] 2\n\n# Shows the number of rows or observations\nnrow(vote)\n\n[1] 3\n\n# Shows both the number of rows (observations and columns\n# (variables).\ndim(vote)\n\n[1] 3 2\n\n\n\n2.4.1 The Assignment Operator in Creating Objects\n\nEntering and Storing Variables in R requires you to make an assignment.\n\nWe use the assignment operator ‘&lt;-’ to assign a value or expression to a variable.\nWe typically do not use the = sign in R even though it works because it also means other things in R.\n\nSome examples are below to add to your .R file.\n\n\nstates &lt;- 29\nA &lt;- \"Apple\"\n# Equivalent statement to above - again = is less used in R.\nA = \"Apple\"\nprint(A)\n\n[1] \"Apple\"\n\n# Equivalent statement to above\nA\n\n[1] \"Apple\"\n\nB &lt;- 3 + 4 * 12\nB\n\n[1] 51\n\n\n\n\n\nThe Assignment Operator\n\n\n\n\n2.4.2 Naming Objects\n\nLine length limit: 80\nAlways use a consistent way of annotating code.\nCamel case is capitalizing the first letter of each word in the object name, with the exception of the first word.\nDot case puts a dot between words in a variable name while camel case capitalizes each word in the variable name.\nObject names appear on the left of assignment operator. We say an object receives or is assigned the value of the expression on the right.\n\n\nNaming Constants: A Constant contains a single numeric value.\n\n\nThe recommended format for constants is starting with a “k” and then using camel case. (e.g., kStates).\n\n\nNaming Functions: Functions are objects that perform a series of R commands to do something in particular.\n\n\nThe recommended format for Functions is to use Camel case with the first letter capitalized. (e.g., MultiplyByTwo).\n\n\nNaming Variables: A Variable is a measured characteristic of some entity.\n\n\nThe recommended format for variables is to use either the dot case or camel case. e.g., filled.script.month or filledScriptMonth.\nA valid variable name consists of letters, numbers, along with the dot or underline characters.\nA variable name must start with a letter, or the dot when not followed by a number.\nA variable cannot contain spaces.\nVariable names are case sensitive: x is different from X just as Age is different from AGE.\nThe value on the right must be a number, string, an expression, or another variable.\nSome Examples Using Variable Rules:\n\n\nAB.1 &lt;- \"Allowed?\"\n# Does not follow rules - not allowed Try the statement below with no\n# hashtag to see the error message .123 &lt;- 'Allowed?'\nA.A123 &lt;- \"Allowed?\"\nG123AB &lt;- \"Allowed?\"\n# Recommended format for constants\nkStates &lt;- 29\n\n\nDifferent R coders have different preferences, but consistency is key in making sure your code is easy to follow and for others to read. In this course, we will generally use the recommendation in the text which are listed above.\nWe tend to use one letter variable names (i.e., x) for placeholders or for simple functions (like naming a vector).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#built-in-functions",
    "href": "introR.html#built-in-functions",
    "title": "2  Introduction to R and RStudio",
    "section": "2.5 Built-in Functions",
    "text": "2.5 Built-in Functions\n\nR has thousands of built-in functions including those for summary statistics. Below, we use a few built-in functions with constant numbers. The sqrt(), max(), and min() functions compute the square root of a number, and find the maximum and minimum numbers in a vector.\n\n\nsqrt(100)\n\n[1] 10\n\nmax(100, 200, 300)\n\n[1] 300\n\nmin(100, 200, 300)\n\n[1] 100\n\n\n\nWe can also create variables to use within built-in functions.\nBelow, we create a vector x and use a few built-in functions as examples.\n\nThe sort() function sorts a vector from small to large.\n\n\nx &lt;- c(1, 2, 3, 3, 100, -10, 40)  #Creating a Vector x\nsort(x)  #Sorting the Vector x from Small to Large\n\n[1] -10   1   2   3   3  40 100\n\nmax(x)  #Finding Largest Element of Vector x\n\n[1] 100\n\nmin(x)  #Finding Smallest Element of Vector x\n\n[1] -10\n\n\n\n\n2.5.1 Built-in Functions: Setting an Argument\n\nThe standard format to a built-in function is functionName(argument)\n\nFor example, the square root function structure is listed as sqrt(x), where x is a numeric or complex vector or array.\n\n\n\n# Here, we are setting a required argument x to a value of 100. When\n# a value is set, it turns it to a parameter of the function.\nsqrt(x = 100)\n\n[1] 10\n\n# Because there is only one argument and it is required, we can\n# eliminate its name x= from our function call. This is discussed\n# below.\nsqrt(100)\n\n[1] 10\n\n\n\nThere is a little variety in how we can write functions to get the same results.\nA parameter is what a function can take as input. It is a placeholder and hence does not have a concrete value. An argument is a value passed during function invocation.\nThere are some default values set up in R in which arguments have already been set.\nThere are a few functions with no parameters like Sys.time() which produces the date and time. If you are not sure how many parameters a function has, you should look it up in the help.\n\n\n\n2.5.2 Default Values\n\nThere are many default values set up in R in which arguments have already been set to a particular value or field.\nDefault values have been set when you see the = value in the instructions. If we don’t want to change it, we don’t need to include it in our function call.\nWhen only one argument is required, the argument is usually not set to have a default value.\n\n\n\n2.5.3 Built-in Functions: Using More than One Argument\n\nFor functions with more than one parameter, we must determine what arguments we want to include, and whether a default value was set and if we want to change it. Default values have been set when you see the = value in the instructions. If we don’t want to change it, we don’t need to include it in our function call.\n\nFor example, the default S3 method for the seq() function is listed as the following: seq(from = 1, to = 1, by = ((to - from)/(length.out - 1)),length.out = NULL, along.with = NULL, …)\nDefault values have been set on each parameter, but we can change some of them to get a meaningful result.\nFor example, we set the from, to, and by parameter to get a sequence from 0 to 30 in increments of 5.\n\n\n\n# We can use the following code.\nseq(from = 0, to = 30, by = 5)\n\n[1]  0  5 10 15 20 25 30\n\n\n\nWe can simplify this function call even further:\n\nIf we use the same order of parameters as the instructions, we can eliminate the argument= from the function.\nSince we do list the values to the arguments in same order as the function is defined, we can eliminate the from=, to=, and by= to simplify the statement.\n\n\n# Equivalent statement as above\nseq(0, 30, 5)\n\n[1]  0  5 10 15 20 25 30\n\n\nIf you leave off the by parameter, it defaults at 1.\n\n\n# Leaving by= to default value of 1\nseq(0, 30)\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n[26] 25 26 27 28 29 30\n\n\n\nThere can be a little hurdle deciding when you need the argument value in the function call. The general rule is that if you don’t know, include it. If it makes more sense to you to include it, include it.\n\n\n\n2.5.4 Tips on Arguments\n\nAlways look up a built-in function to see the arguments you can use.\nArguments are always named when you define a function.\nWhen you call a function, you do not have to specify the name of the argument.\nArguments have default values, which is used if you do not specify a value for that argument yourself.\nAn argument list comprises of comma-separated values that contain the various formal arguments.\nDefault arguments are specified as follows: parameter = expression\n\n\ny &lt;- 10:20\nsort(y)\n\n [1] 10 11 12 13 14 15 16 17 18 19 20\n\nsort(y, decreasing = FALSE)\n\n [1] 10 11 12 13 14 15 16 17 18 19 20",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#saving",
    "href": "introR.html#saving",
    "title": "2  Introduction to R and RStudio",
    "section": "2.6 Saving",
    "text": "2.6 Saving\n\nYou can save your work in the file menu or the save shortcut using Ctrl + S or Cmd+S depending on your Operating System.\nYou will routinely be asked to save your workspace image, and you don’t need to save this unless specifically asked. It saves the output we have generated so far.\nYou can stop this from happening by setting the Tools &gt; Global Options &gt; Under Workspace changing this to Never.\nBe careful with this option because it won’t save what you don’t run.\n\n\n\n\nEvalulating Your Environment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#calling-a-library",
    "href": "introR.html#calling-a-library",
    "title": "2  Introduction to R and RStudio",
    "section": "2.7 Calling a Library",
    "text": "2.7 Calling a Library\n\nIn R, a package is a collection of R functions, data and compiled code. The location where the packages are stored is called the library.\nLibraries need to be activated one time in each new R session.\nYou can access a function from a library one time using library::function()\n\n\n# Use to activate library in an R session.\nlibrary(tidyverse)\nlibrary(dplyr)\n\n\nYou can access a function from a library one time only using library::function()\n\nUseful if only using one function from the library.\n\nWe will return to this in data prep.\n\n\n## Below is an example that would use dplyr for one select function\n## to select variable1 from the oldData and save it as a new object\n## NewData. Since we don’t have datasets yet, we will revisit this.\n## NewData &lt;- dplyr::select(oldData, variable1)\n\n\nSome libraries are part of other global libraries:\n\ndplyr is part of tidyverse, there is actually no need to activate it if tidyverse is active, however, sometimes it helps when conflicts are present\nAn example of a conflict is the use of a select function which shows up in both the dplyr and MASS package. If both libraries are active, R does not know which to use.\ntidyverse has many libraries included in it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#the-environment",
    "href": "introR.html#the-environment",
    "title": "2  Introduction to R and RStudio",
    "section": "2.8 The Environment",
    "text": "2.8 The Environment\n\nYou can evaluate your Environment Tab to see your Variables we have defined in R Studio.\nUse the following functions to view and remove defined variables in your Global Environment\n\n\nls()  #Lists all variables in Global Environment \n\n [1] \"A\"       \"A.A123\"  \"AB.1\"    \"B\"       \"G123AB\"  \"income\"  \"kStates\"\n [8] \"states\"  \"vote\"    \"voted\"   \"x\"       \"y\"      \n\nrm(states)  #Removes variable named states\nrm(list = ls())  #Clears all variables from Global Environment\n\n\n\n\nEvalulating Your RStudio Environment",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#entering-and-loading-data",
    "href": "introR.html#entering-and-loading-data",
    "title": "2  Introduction to R and RStudio",
    "section": "2.9 Entering and Loading Data",
    "text": "2.9 Entering and Loading Data\n\n2.9.1 Creating a Vector\n\nA vector is the simplest type of data structure in R.\n\nA vector is a set of data elements that are saved together as the same type.\nWe have many ways to create vectors with some examples below.\n\nUse c() function, which is a generic function which combines its arguments into a vector or list.\n\n\nc(1, 2, 3, 4, 5)  #Print a Vector 1:5\n\n[1] 1 2 3 4 5\n\n\n\nIf numbers are aligned, can use the “:“ symbol to include numbers and all in between. This is considered an array.\n\n\n1:5  #Print a Vector 1:5\n\n[1] 1 2 3 4 5\n\n\n\nUse seq() function to make a vector given a sequence.\n\n\nseq(from = 0, to = 30, by = 5)  #Creates a sequence vector from 0 to 30 in increments on 5 \n\n[1]  0  5 10 15 20 25 30\n\n\n\nUse rep() function to repeat the elements of a vector.\n\n\nrep(x = 1:3, times = 4)  #Repeat the elements of the vector 4 times\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3\n\nrep(x = 1:3, each = 3)  #Repeat the elements of a vector 3 times each\n\n[1] 1 1 1 2 2 2 3 3 3\n\n\n\n\n2.9.2 Creating a Matrix\n\nA matrix is another type of object like a vector or a list.\n\nA matrix has a rectangular format with rows and columns.\nA matrix uses matrix() function\nYou can include the byrow = argument to tell the function whether to fill across or down first.\nYou can also include the dimnames() function in addition to the matrix() to assign names to rows and columns.\n\nUsing matrix() function, we can create a matrix with 3 rows and 3 columns as shown below.\n\nTake note how the matrix fills in the new data.\n\n\n# Creating a Variable X that has 9 Values.\nx &lt;- 1:9\n# Setting the matrix.\nmatrix(x, nrow = 3, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n# Note – we do not need to name the arguments because we go in the\n# correct order.  The function below simplifies the statement and\n# provides the same answer as above.\nmatrix(x, 3, 3)\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n\n2.9.2.1 Setting More Arguments in a Matrix\n\nThe byrow argument fills the Matrix across the row\nBelow, we can use the byrow statement and assign it to a variable m.\n\n\nm &lt;- matrix(1:9, 3, 3, byrow = TRUE)  #Fills the Matrix Across the Row and assigns it to variable m\nm  #Printing the matrix in the console\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\nThe dimnames() function adds labels to either the row and the column. In this case below both are added to our matrix m.\n\n\ndimnames(x = m) &lt;- list(c(\"2020\", \"2021\", \"2022\"), c(\"low\", \"medium\", \"high\"))\nm  #Printing the matrix in the console\n\n     low medium high\n2020   1      2    3\n2021   4      5    6\n2022   7      8    9\n\n\n\nYou try to make a matrix of 25 items, or a 5 by 5, and fill the matrix across the row and assign the matrix to the name m2.\nYou should get the answer below.\n\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15\n[4,]   16   17   18   19   20\n[5,]   21   22   23   24   25\n\n\n\n\n2.9.2.2 Differences between Data Frames and Matrices\n\nIn a data frame the columns contain different types of data, but in a matrix all the elements are the same type of data. A matrix is usually numbers.\nA matrix can be looked at as a vector with additional methods or dimensions, while a data frame is a list.\n\n\n\n\n2.9.3 Creating a Data Frame\n\nA data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column. In a data frame the rows are observations and columns are variables.\n\nData frames are generic data objects to store tabular data.\nThe column names should be non-empty.\nThe row names should be unique.\nThe data stored in a data frame can be of numeric, factor or character type.\nEach column should contain same number of data items.\nCombing vectors into a data frame using the data.frame() function\n\nBelow, we can create vectors for state, year enacted, personal oz limit medical marijuana.\n\n\nstate &lt;- c(\"Alaska\", \"Arizona\", \"Arkansas\")\nyear.legal &lt;- c(1998, 2010, 2016)\nounce.lim &lt;- c(1, 2.5, 3)\n\n\nThen, we can combine the 3 vectors into a data frame and name the data frame pot.legal.\n\n\npot.legal &lt;- data.frame(state, year.legal, ounce.lim)\n\n\nNext, check your global environment to confirm data frame was created.\n\n\n\n\nGlobal Environment\n\n\n\n\n2.9.4 Importing a Data Frame into R\n\nWhen importing data from outside sources, you can do the following:\n\n\nYou can import data from an R package using data() function.\nYou can also link directly to a file on the web.\nYou can import data through from your computer through common file extensions:\n\n.csv: comma separated values;\n.txt: text file;\n.xls or .xlsx: Excel file;\n.sav: SPSS file;\n.sasb7dat: SAS file;\n.xpt: SAS transfer file;\n.dta: Stata file.\n\n\n\nEach different file type requires a unique function to read in the file. With all the variety in file types, it is best to look it up in the R Community to help.\n\n\n2.9.4.1 Use data() function\n\nAll we need is the data() function to read in a data set that is part of R. R has many built in libraries now, so there are many data sets we can use for testing and learning statistics in R.\n\n\n# The mtcars data set is part of R, so no new package needs to be\n# downloaded.\ndata(\"mtcars\")\n\n\nLoad a data frame from a unique package in R.\n\nThere are also a lot of packages that house data sets. It is fairly easy to make a package that contains data and load it into CRAN. These packages need to be installed into your R one time. Then, each time you open R, you need to reload the library using the library() function.\nWhen your run the install.packages() function, do not include the # symbol. Then, after you run it one time, comment it out. There is no need to run this code a second time unless something happens to your RStudio.\n\n\n# install.packages('MASS') #only need to install package one time in\n# R\nlibrary(MASS)\n\n\n\ndata(\"Insurance\")\nhead(Insurance)\n\n  District  Group   Age Holders Claims\n1        1    &lt;1l   &lt;25     197     38\n2        1    &lt;1l 25-29     264     35\n3        1    &lt;1l 30-35     246     20\n4        1    &lt;1l   &gt;35    1680    156\n5        1 1-1.5l   &lt;25     284     63\n6        1 1-1.5l 25-29     536     84\n\n\n\n\n2.9.4.2 Accessing Variables\n\nYou can directly access a variable from a dataset using the $ symbol followed by the variable name.\nThe $ symbol facilitates data manipulation operations by allowing easy access to variables for calculations, transformations, or other analyses. For example:\n\n\nhead(Insurance$Claims)  #lists the first 6 Claims in the Insurance dataset.\n\n[1]  38  35  20 156  63  84\n\nsd(Insurance$Claims)  #provides the standard deviation of all Claims in the Insurance dataset.\n\n[1] 71.1624\n\n\n\n\n2.9.4.3 Setting up a Working Directory\n\nYou should have the data files from our LMS in a data folder on your computer. Your project folder would contain that data folder.\nBefore importing and manipulating data, you must find and edit your working directory to directly connect to your project folder!\nThese functions are good to put at the top of your R files if you have many projects going at the same time.\n\n\ngetwd()  #Alerts you to what folder you are currently set to as your working directory\n# For example, my working directory is set to the following:\n# setwd('C:/Users/Desktop/ProbStat') #Allows you to reset the working\n# directory to something of your choice.\n\n\nIn R, when using the setwd() function, notice the forward slashes instead of backslashes.\nYou can also go to Tools &gt; Global Options &gt; General and reset your default working directory when not in a project. This will pre-select your working directory when you start R.\nOr if in a project, like we should be, you can click the More tab as shown in the Figure below, and set your project folder as your working directory.\n\n\n\n\nSetting Your Working Directory\n\n\n\n\n2.9.4.4 Reading in Data from .csv\n\nReading in a .csv file is extremely popular way to read in data.\nThere are a few functions to read in .csv files. And these functions would change based on the file type you are importing.\n\n\n2.9.4.4.1 read.csv() function\n + Extremely popular way to read in data.\n + read.csv() is a base R function that comes built-in with R: No library necessary\n\nAll your datasets should be in a data folder in your working directory so that you and I have the same working directory.\nStructure of function datasetName &lt;- read.csv(“data/dataset.csv”)\n\n\ngss.2016 &lt;- read.csv(file = \"data/gss2016.csv\")\n# or equivalently\ngss.2016 &lt;- read.csv(\"data/gss2016.csv\")\n# Examine the contents of the file\nsummary(object = gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n# Or equivalently, we can shorten this to the following code\nsummary(gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\n\n\n\n2.9.4.5 Using tidyverse to load data\n\n2.9.4.5.1 read_csv function\n\nread_csv() is a function from the readr package, which is part of the tidyverse ecosystem.\nread_csv() is generally faster than read.csv() as it’s optimized for speed, making it more efficient, particularly for large datasets.\n\n\n# install.packages(tidyverse) ## Only need to install one time on\n# your computer. #install.packages links have been commented out\n# during processing of RMarkdown.  Activate the library, which you\n# need to access each time you open R and RStudio\nlibrary(tidyverse)\n\n\n# Now open the data file to evaluate with tidyverse\ngss.2016b &lt;- read_csv(file = \"data/gss2016.csv\")\n\n\n\n\n\n2.9.5 Summarize Data\n\nUse the summary() function to examine the contents of the file.\nThe summary() function is part of base R and does not require a package.\n\n\nsummary(object = gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\n\nAgain, we can eliminate the object = because it is the first argument and is required.\n\n\nsummary(gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\n\n\n2.9.5.1 Explicit Use of Libraries\n\nYou can activate a library one time using library::function() format\nFor example, we can use the summarize() function from dplyr which is part of tidyverse installed earlier.\nSince dplyr is part of tidyverse, there is actually no need to activate it when we have already activated tidyverse in this session, however, it does help when conflicts are present. More on that later.\n\nThe line below says to take the the gss.2016 data object and summarize the length of age using the dplyr library.\n\n\ndplyr::summarize(gss.2016, length.age = length(age))\n\n  length.age\n1       2867\n\n\nIn the line of code above, we see package::function(). If we initiate the library like below, we do not need the beginning of the statement. The code below provides the same answer as the way written above.\n\n\nlibrary(dplyr)\nsummarize(gss.2016, length.age = length(age))\n\n  length.age\n1       2867\n\n\n\nYou try to access the ChickWeight dataset from the MASS package and summarize it generally using summary() and str() functions.\nEarlier, we looked up the tapply() function in the help bar and found that the format is tapply(x, index, and fun), where x is a continuous variable, index is a grouping variable or factor, and fun is a function like mean. Use the tapply() function to take the mean weight of Chicks based on Diet. See the answer below.\nWhile there are a few ways to get group means in R. I like the tapply() function which is used to apply a function over subsets of a vector. The function splits the data into subsets based on a given factor or factors, applies a specified function to each subset, and then returns the results in a convenient form. Here, we are applying the mean function.\nTo access a variable, use dataset$variable or you can also attach a dataset to your code so you can just use the variable name later on. So use ChickWeight$weight and ChickWeight$Diet appropriately in the function alongside the FUN mean.\n\n\n\n     weight           Time           Chick     Diet   \n Min.   : 35.0   Min.   : 0.00   13     : 12   1:220  \n 1st Qu.: 63.0   1st Qu.: 4.00   9      : 12   2:120  \n Median :103.0   Median :10.00   20     : 12   3:120  \n Mean   :121.8   Mean   :10.72   10     : 12   4:118  \n 3rd Qu.:163.8   3rd Qu.:16.00   17     : 12          \n Max.   :373.0   Max.   :21.00   19     : 12          \n                                 (Other):506          \n\n\n       1        2        3        4 \n102.6455 122.6167 142.9500 135.2627",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "introR.html#summary",
    "href": "introR.html#summary",
    "title": "2  Introduction to R and RStudio",
    "section": "2.10 Summary",
    "text": "2.10 Summary\n\nIn this lesson, we went over information to make sure you had some basics to really start learning R. There are a lot of ways to get the same things done in R; you have to find the way that works best for you. As we learn R, you will get used to doing things your way to be able to slice and evaluate the data to find rich information from the data sets we look at. As long as the data was handled properly, it does not matter how we reach our goal using R as long as we do it ourselves.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R and RStudio</span>"
    ]
  },
  {
    "objectID": "descriptives.html",
    "href": "descriptives.html",
    "title": "3  Descriptive Statistics",
    "section": "",
    "text": "3.0.1 Lesson Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#summarizing-qualitative-data",
    "href": "descriptives.html#summarizing-qualitative-data",
    "title": "3  Descriptive Statistics",
    "section": "3.1 Summarizing Qualitative Data",
    "text": "3.1 Summarizing Qualitative Data\n\nQualitative data is information that cannot be easily counted, measured, or easily expressed using numbers.\n\nNominal variables: a type of categorical variable that represents discrete categories or groups with no inherent order or ranking\n\ngender (male, female)\nmarital status (single, married, divorced)\neye color (blue, brown, green)\n\nOrdinal variables: categories possess a natural order or ranking\n\na Likert scale measuring agreement with a statement (e.g., strongly disagree, disagree, neutral, agree, strongly agree)\n\n\nA frequency distribution shows the number of observations in each category for a factor or categorical variable.\nGuidelines when constructing frequency distribution:\n\nClasses or categories are mutually exclusive (they are all unique).\nClasses or categories are exhaustive (a full list of categories).\n\nTo calculate frequencies, first, start with a variable that has categorical data.\n\n\n# Create a vector with some data that could be categorical\nSample_Vector &lt;- c(\"A\", \"B\", \"A\", \"C\", \"A\", \"B\", \"A\", \"C\", \"A\", \"B\")\n# Create a data frame with the vector\ndata &lt;- data.frame(Sample_Vector)\n\n\nTo count the number of each category value, we can use the table() command.\nThe output shows a top row of categories and a bottom row that contains the number of observations in the category.\n\n\n# Create a table of frequencies\nfrequencies &lt;- table(data$Sample_Vector)\nfrequencies\n\n\nA B C \n5 3 2 \n\n\n\nRelative frequency is how often something happens divided by all outcomes.\nThe relative frequency is calculated by \\(f_i/n\\), where \\(f_i\\) is the frequency of class \\(i\\) and \\(n\\) is the total frequency.\nWe can use the prop.table() command to calculate relative frequency by dividing each category’s frequency by the sample size.\n\n\n# Calculate proportions\nproportions &lt;- prop.table(frequencies)\n\n\nThe cumulative relative frequency is given by \\(cf_i/n\\), where \\(cf_i\\) is the cumulative frequency of class \\(i\\).\nThe cumsum() function calculates the cumulative distribution of the data\n\n\n# Calculate cumulative frequencies\ncumulfreq &lt;- cumsum(frequencies)\n# Calculate cumulative proportions\ncumulproportions &lt;- cumsum(prop.table(frequencies))\n\n\nThe rbind() function is used to combine multiple data frames or matrices by row. The name “rbind” stands for “row bind”. Since the data produced by the table is in rows, we can use rbind to link them together.\n\n\n# combine into table\nfrequency_table &lt;- rbind(frequencies, proportions, cumulfreq, cumulproportions)\n# Print the table\nfrequency_table\n\n                   A   B    C\nfrequencies      5.0 3.0  2.0\nproportions      0.5 0.3  0.2\ncumulfreq        5.0 8.0 10.0\ncumulproportions 0.5 0.8  1.0\n\n\n\nWe can transpose a table using the t() command, which flips the dataset.\n\n\nTransposedData &lt;- t(frequency_table)\nTransposedData\n\n  frequencies proportions cumulfreq cumulproportions\nA           5         0.5         5              0.5\nB           3         0.3         8              0.8\nC           2         0.2        10              1.0\n\n\n\nFinally, sometimes we need to transform our calculations into a dataset.\nThe as.data.frame function is used to coerce or convert an object into a data frame\n\n\nTransposedData &lt;- as.data.frame(TransposedData)\nTransposedData\n\n  frequencies proportions cumulfreq cumulproportions\nA           5         0.5         5              0.5\nB           3         0.3         8              0.8\nC           2         0.2        10              1.0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#summarizing-quantitative-data",
    "href": "descriptives.html#summarizing-quantitative-data",
    "title": "3  Descriptive Statistics",
    "section": "3.2 Summarizing Quantitative Data",
    "text": "3.2 Summarizing Quantitative Data\n\n3.2.1 Defining and Calculating Central Tendency\n\nThe term central location refers to how numerical data tend to cluster around some middle or central value.\nMeasures of central location attempt to find a typical or central value that describes a variable.\nWhy frequency distributions do not work for numeric variables:\n\nNumeric variables measured on a continuum.\nInstead, we calculate descriptive statistics including central tendency and spread of the values for a numeric variable.\n\nWe will examine the three mostly widely used measures of central location: mean, median and mode.\nThen we discuss a percentile: a measure of relative position.\n\n\n3.2.1.1 Using the Mean\n\nThe arithmetic mean or simply the mean is a primary measure of central location. It is often referred to as the average. Simply add up all the observations and divide by the number of observations.\nThe numerator (top of the fraction) is the sum (sigma) of all the values of x from the first value (i = 1) to the last value (n) divided by the number of values (n).\n\\(m_x = (\\sum_{i=1}^{n} x_{i})/n\\)\nConsider the salaries of employees at a company: \nWe can use the mean() command to calculate the mean in R.\n\n\n# Create Vector of Salaries\nsalaries &lt;- c(40000, 40000, 65000, 90000, 145000, 150000, 550000)\n# Calculate the mean using the mean() command\nmean(salaries)\n\n[1] 154285.7\n\n\n\nNote that due to at least one outlier this mean does not reflect the typical salary - more on that later.\nIf we edit our vector to include NAs, we have to account for this. This is a common way to handle NAs in functions that do not allow for them.\n\n\nsalaries2 &lt;- c(40000, 40000, 65000, 90000, 145000, 150000, 550000, NA,\n    NA)\n# Calculate the mean using the mean() command Notice that it does not\n# work\nmean(salaries2)\n\n[1] NA\n\n# Add in na.rm parameter to get it to produce the mean with no NAs.\nmean(salaries2, na.rm = TRUE)\n\n[1] 154285.7\n\n\n\nNote that there are other types of means like the weighted mean or the geometric mean.\n\nThe weighted mean uses weights to determine the importance of each data point of a variable. It is calculated by \\(\\bar{x}_w = \\frac{\\sum_{i=1}^{n} w_i x_i}{\\sum_{i=1}^{n} w_i}\\), where are the weights associated to the values.\nAn example is below.\n\n\nvalues &lt;- c(4, 7, 10, 5, 6)\nweights &lt;- c(1, 2, 3, 4, 5)\nweighted_mean &lt;- weighted.mean(values, weights)\nweighted_mean\n\n[1] 6.533333\n\n\n\n\n3.2.1.2 Using the Median\n\nThe median is another measure of central location that is not affected by outliers.\nWhen the data are arranged in ascending order, the median is:\n\nThe middle value if the number of observations is odd, or\nThe average of the two middle values if the number of observations is even.\n\nConsider the sorted salaries of employees presented earlier which contains an odd number of observations.\n\nOn the same salaries vector created above, use median() command to calculate the median in R.\n\n\n# Calculate the median using the median() command\nmedian(salaries)\n\n[1] 90000\n\n\n\nNow compare to the mean and note the large difference in numbers signifying that at least one outlier is most likely present.\nSpecifically, if the mean and median are different, it is likely the variable is skewed and contains outliers.\n\n\nmean(salaries)\n\n[1] 154285.7\n\n\n\nFor another example, consider the sorted data below that contains an even number of values.\n\n\nGrowthFund &lt;- c(-38.32, 1.71, 3.17, 5.99, 12.56, 13.47, 16.89, 16.96, 32.16,\n    36.29)\n\n\nWhen data contains an even number of values, the median is the average of the 2 sorted middle numbers (12.56 and 13.47).\n\n\nmedian(GrowthFund)\n\n[1] 13.015\n\n(12.56 + 13.47)/2\n\n[1] 13.015\n\n# The mean is still the average\nmean(GrowthFund)\n\n[1] 10.088\n\n\n\n\n3.2.1.3 Using the Mode\n\nThe mode is another measure of central location.\nThe mode is the most frequently occurring value in a data set.\nThe mode is useful in summarizing categorical data.\nA data set can have no mode, one mode (unimodal), two modes (bimodal) or many modes (multimodal).\nThe mode is less useful when there are more than three modes.\nThe mode is useful summary for a categorical variable.\n\n\n\n3.2.1.4 Example of Function with Salary Variable\n\nConsider the salary of employees presented earlier. The mode is $40,000 since this value appears most often.\n\n\n# Try this command with and without it.\nnames(sort(x = table(salaries), decreasing = TRUE))\n\n[1] \"40000\"  \"65000\"  \"90000\"  \"145000\" \"150000\" \"550000\"\n\n\n\n40,000 appears 2 times and is the mode because that occurs most often.\n\n\n\n3.2.1.5 Finding No Mode\n\nLook at the sort(table()) commands with the GrowthFund Vector we made earlier.\nI added a [1:3] at the end of the statement to produce the 3 highest frequencies found in the vector.\n\n\nsort(table(GrowthFund), decreasing = TRUE)[1:3]\n\nGrowthFund\n-38.32   1.71   3.17 \n     1      1      1 \n\n\n\nEven if you use this command, you still need to evaluate the data more systematically to verify the mode. If the highest frequency of the sorted table is 1, then there is no mode.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#defining-and-calculating-spread",
    "href": "descriptives.html#defining-and-calculating-spread",
    "title": "3  Descriptive Statistics",
    "section": "3.3 Defining and Calculating Spread",
    "text": "3.3 Defining and Calculating Spread\n\nSpread is a measure of distance values are from the central value.\nEach measure of central tendency has one or more corresponding measures of spread.\nMean: use variance or standard deviation to measure spread.\n\nskewness and kurtosis help measure spread as well.\n\nMedian: use range or interquartile range (IQR) to measure spread.\nMode: use the index of qualitative variation to measure spread.\n\nNot formally testing here with a function.\n\n\n\n3.3.1 Spread to Report with the Mean\n\n3.3.1.1 Evaluating Skewness\n\nSkewness is a measure of the extent to which a distribution is skewed.\nCan evaluate skewness visually with histogram.\n\nA histogram is a visual representation of a frequency or a relative frequency distribution.\nBar height represents the respective class frequency (or relative frequency).\nBar width represents the class width.\n\n\n\n\n\nEvaluating Skewness Visually\n\n\n\n\n3.3.1.2 Skewed Distributions: Median Not Same as Mean\n\nSometimes, a histogram is difficult to tell if skewness is present or if the data is relatively normal or symmetric.\nIf Mean is less than Median and Mode, then the variable is Left-Skewed.\nIf the Mean is greater than the Median and Mode, then the variable is Right-Skewed.\nIf the Mean is about equal to the Median and Mode, then the variable has a symmetric distribution.\nIn R, we can easily look at mean and median with the summary() command.\n\n\n\n\nEvaluating Skewness Using Mean and Median\n\n\n\nMean is great when data are normally distributed (data is not skewed).\nMean is not a good representation of skewed data where outliers are present.\n\nAdding together a set of values that includes a few very large or very small values like those on the far left of a left-skewed distribution or the far right of the right-skewed distribution will result in a large or small total value in the numerator of Equation and therefore the mean will be a large or small value relative to the actual middle of the data.\n\n\n\n\n3.3.1.3 Using skew() Command in R\n\nThe skew() command is from the semTools package. The install.packages() command is commented out below, but install it one time on your R before commenting it out.\n\n\n# install the semTools package if necessary.\n# install.packages('semTools') Activate the library\nlibrary(semTools)\n\n\nAfter the package is installed and loaded, run the skew() command on the salaries vector made above.\n\n\nskew(salaries)\n\n skew (g1)         se          z          p \n2.31126775 0.92582010 2.49645450 0.01254418 \n\n\n\n\n3.3.1.4 Interpreting the skew() Command Results\n\nse = standard error\nz = skew/se\nIf the sample size is small (n &lt; 50), z values outside the –2 to 2 range are a problem.\nIf the sample size is between 50 and 300, z values outside the –3.29 to 3.29 range are a problem.\nFor large samples (n &gt; 300), using a visual is recommended over the statistics, but generally z values outside the range of –7 to 7 can be considered problematic.\nSalary: Our sample size was small, &lt;50, so the z value of 2.496 in regards to the salary vector indicates there is a problem with skewness.\nGrowthFund: We can check the skew of GrowthFund.\n\n\nskew(GrowthFund)\n\n  skew (g1)          se           z           p \n-1.38071963  0.77459667 -1.78250137  0.07466751 \n\n\n\nGrowthFund was also considered a small sample size, so the same -2/2 thresholds are used. Here, our z value is -1.78250137, which is in normal range. This indicates there is no problem with skewness.\n\n\n\n\n3.3.2 Histograms\n\nA histogram is a graphical representation of the distribution of numerical data.\nIt consists of a series of contiguous rectangles, or bars, where the area of each bar corresponds to the frequency of observations within a particular range or bin of values.\nThe x-axis typically represents the range of values being measured, while the y-axis represents the frequency or count of observations falling within each range.\nHistograms are commonly used in statistics and data analysis to visualize the distribution of a dataset and identify patterns or trends.\nThey are particularly useful for understanding the central tendency, variability, and shape of the data distribution - this includes our observation of skewness.\nWorks much better with larger datsets.\n\n\n3.3.2.1 Commands to Make a Histogram\n\nhist() command in base R.\ngeom_histogram() command in ggplot2 package.\na hist using the GrowthFund dataset does not look that great because its sample size is so small.\n\n\nhist(GrowthFund)\n\n\n\n\n\n\n\n\n\n\n3.3.2.2 hist vs geom_histogram\n\nIn R, hist() and geom_histogram() are both used to create histograms, but they belong to different packages and have slightly different functionalities.\n\n\n# Making an appropriate data.frame to use the hist() command\nHousePrice &lt;- c(430, 520, 460, 475, 670, 521, 670, 417, 533, 525, 538,\n    370, 530, 525, 430, 330, 575, 555, 521, 350, 399, 560, 440, 425, 669,\n    660, 702, 540, 460, 588, 445, 412, 735, 537, 630, 430)\nHousePrice &lt;- data.frame(HousePrice)\n\n\nhist(): This function is from the base R graphics package and is used to create histograms. It provides a simple way to visualize the distribution of a single variable.\n\n\n# Using base R to create the histogram.\nhist(HousePrice$HousePrice, breaks = 5, main = \"A Histogram\", xlab = \"House Prices (in $1,000s)\",\n    col = \"yellow\")\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\n\ngeom_histogram(): This function is from the ggplot2 package, which is part of the tidyverse. It is used to create histograms as part of a more flexible and powerful plotting system.\n\n\n# Using geom_histogram() command to create the histogram.\nggplot(HousePrice, aes(x = HousePrice)) + geom_histogram(binwidth = 100,\n    boundary = 300, color = \"black\", fill = \"yellow\") + labs(title = \"A Histogram\",\n    x = \"House Prices (in $1,000s)\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nWe could add more parameters here to make the 2 histograms look identical, but this configuration of parameters is very close. Take note that there are a lot more parameters you can add to the geom_histogram() command than you can with base R to make it look more professional. Be sure to look them up and also check with the notes in the book, which focuses on geom_histogram instead of hist().\nVariance is a measure of spread for numeric variables that is essentially the average of the squared differences between each observation value on some variable and the mean for that variable with population variance. \\[Population Var(X) = \\sigma^2 = \\sum{(x_i-\\mu)^2}/N\\] \\[Sample Var(x) = s^2 = \\sum{(x_i-\\bar{x})^2}/(n-1)\\]\nStandard deviation is the square root of the variance.\n\nUse var() command and sd() command to calculate sample variance and sample standard deviation.\n\n\n\n## Calculated from Small Sample\nx &lt;- c(1, 2, 3, 4, 5)\nsum((x - mean(x))^2/(5 - 1))\n\n[1] 2.5\n\nvar(x)\n\n[1] 2.5\n\nsqrt(var(x))\n\n[1] 1.581139\n\nsd(x)\n\n[1] 1.581139\n\nsd(HousePrice$HousePrice)\n\n[1] 102.6059\n\nvar(HousePrice$HousePrice)\n\n[1] 10527.97\n\n\nLooking at Spread for a Larger Dataset\n\n\ncustomers &lt;- read.csv(\"data/customers.csv\")\nsummary(customers$Spending, na.rm = TRUE)  #mean and median\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   50.0   383.8   662.0   659.6   962.2  1250.0 \n\n### Spread to Report with the Mean\nsd(customers$Spending, na.rm = TRUE)\n\n[1] 350.2876\n\nvar(customers$Spending, na.rm = TRUE)\n\n[1] 122701.4\n\n\n\n\n3.3.2.3 Kurtosis in Evaluating Mean Spread\n\nKurtosis is the sharpness of the peak of a frequency-distribution curve or more formally a measure of how many observations are in the tails of a distribution.\nA normal distribution will have a kurtosis value of three;\n\nDistributions with kurtosis around 3 are described as mesokurtic.\nIf kurtosis is significantly above or below 3, there is excess kurtosis.\n\nValues of kurtosis significantly above 3 indicate the distribution is leptokurtic, with fewer observations in the tails than a normal distribution (the fewer observations in the tails often give a distribution a pointy look).\nValues of kurtosis significantly below 3 indicate the distribution is platykurtic, with more observations in the tails than a normal distribution would have given the mean, standard deviation, and sample size.\n\nUses kurtosis() command from the semTools package.\n\n\n\n\n\nEvaluate Kurtosis\n\n\n\nThe kurtosis() command subtracts 3 from the kurtosis, so positive values will be indicative to a leptokurtic distribution and negative will indicate a platykurtic distribution. To see if kurtosis (leptokurtic or platykurtic) is significant, we confirm them by first evaluating the z-score to see if the variable is normal or not. The same cutoff values from skew also apply for the z for small, medium, and large sample sizes in kurtosis. These are the same basic rules for the rules in judging skewness.\n\n\n# z-value is 3.0398, which is &gt; 2 indicating leptokurtic Small sample\n# size: range is -2 to 2\nkurtosis(salaries)\n\nExcess Kur (g2)              se               z               p \n    5.628711065     1.851640200     3.039851407     0.002366949 \n\n# z-value is 2.20528007, which is &gt; 2 indicating leptokurtic Small\n# sample size: range is -2 to 2\nkurtosis(GrowthFund)\n\nExcess Kur (g2)              se               z               p \n     3.41640519      1.54919334      2.20528007      0.02743445 \n\n# Small sample size: range is -2 to 2 Skewness and kurtosis are both\n# in range.\nskew(HousePrice$HousePrice)  #normal\n\nskew (g1)        se         z         p \n0.3173182 0.4082483 0.7772676 0.4370009 \n\nkurtosis(HousePrice$HousePrice)  #normal\n\nExcess Kur (g2)              se               z               p \n     -0.5399982       0.8164966      -0.6613601       0.5083814 \n\n\n\nThe rules of determining problematic distributions with regards to kurtosis are below.\n\nIf the sample size is small (n &lt; 50), z values outside the –2 to 2 range are a problem.\nIf the sample size is between 50 and 300, z values outside the –3.29 to 3.29 range are a problem.\nFor large samples (n &gt; 300), using a visual is recommended over the statistics, but generally z values outside the range of –7 to 7 can be considered problematic.\nIf kurtosis is found, then evaluate the excess kur score to see if it is positive or negative to determine whether it is leptokurtic or platykurtic.\n\nLet’s do a few more examples using the customers dataset.\n\n\n# Noted sample size at 200 observations or a medium sample size.\n# Using threshold –3.29 to 3.29 to assess normality.\n\n#-3.4245446445 is below -3.29 so kurtosis is present\n# Negative kurtosis value indicates platykurtic\nkurtosis(customers$Spending)\n\nExcess Kur (g2)              se               z               p \n  -1.1862970634    0.3464101615   -3.4245446445    0.0006158307 \n\n# Normal: 2.977622119 is in between -3.29 and 3.29\nkurtosis(customers$Income)\n\nExcess Kur (g2)              se               z               p \n    1.031478559     0.346410162     2.977622119     0.002904939 \n\n#-3.7251961028 is below -3.29 so kurtosis is present\n# Negative kurtosis value indicates platykurtic\nkurtosis(customers$HHSize)\n\nExcess Kur (g2)              se               z               p \n  -1.2904457837    0.3464101615   -3.7251961028    0.0001951634 \n\n# Normal: -0.20056607 is in between -3.29 and 3.29\nkurtosis(customers$Orders)\n\nExcess Kur (g2)              se               z               p \n    -0.06947812      0.34641016     -0.20056607      0.84103789 \n\n\n\n\n\n3.3.3 Spread to Report with the Median\n\nRange = Maximum Value – Minimum Value.\n\nSimplest measure.\nFocuses on Extreme values.\nUse commands diff(range()) or max() – min().\n\nIQR: Difference between the first and third quartiles.\n\nUse IQR() command or quantile() command.\n\n\nsummary(customers$Spending, na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   50.0   383.8   662.0   659.6   962.2  1250.0 \n\ndiff(range(customers$Spending, na.rm = TRUE))\n\n[1] 1200\n\nmax(customers$Spending, na.rm = TRUE) - min(customers$Spending, na.rm = TRUE)\n\n[1] 1200\n\nIQR(customers$Spending, na.rm = TRUE)\n\n[1] 578.5\n\n\n\n\n\n3.3.4 Spread to Report with the Mode\n\nWhile there is no great function to test for spread, you can look at the data and see if it is concentrated around 1 or 2 frequencies. If it is, then the spread is distorted towards those high frequency values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#using-ai",
    "href": "descriptives.html#using-ai",
    "title": "3  Descriptive Statistics",
    "section": "3.4 Using AI",
    "text": "3.4 Using AI\nUse the following prompts on a generative AI, like chatGPT to learn more about descriptive statistics. + Explore and understand the distribution of data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "descriptives.html#summary",
    "href": "descriptives.html#summary",
    "title": "3  Descriptive Statistics",
    "section": "3.5 Summary",
    "text": "3.5 Summary\n\nIn this lesson, we worked through descriptive statistics including skewness and kurtosis. We learned about variables and scales of measurement, how to summarize qualitative and quantitative data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "dataprep.html",
    "href": "dataprep.html",
    "title": "4  Data Preparation",
    "section": "",
    "text": "4.0.1 At a Glance",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#evaluating-data-types",
    "href": "dataprep.html#evaluating-data-types",
    "title": "4  Data Preparation",
    "section": "4.1 Evaluating Data Types",
    "text": "4.1 Evaluating Data Types\n\nThere are a number of data types in R that are common to programming and statistical analysis.\nA data type of a variable specifies the type of data that is stored inside that variable. Sometimes when you read in data it is in the correct type, and other times, you need to force it into the type you need to conduct the analysis. In this section, we are going to go over the following data types.\n\n\n\n4.1.1 Factor data type:\n\nOrdinal: Contain categories that have some logical order (e.g. categories of age).\nNominal: Have categories that have no logical order (e.g., religious affiliation and marital status).\nR will treat each unique value of a factor as a different level.\n\n\n4.1.1.1 Ordinal Variable\n\nOrdinal data may be categorized and ranked with respect to some characteristic or trait.\nFor example, instructors are often evaluated on an ordinal scale (excellent, good, fair, poor).\nThis scale allows us to code the data based on order, assuming equal distance between scale items (aka likert items).\nYou can make an ordinal factor data type in R, or you can convert the order to meaningful numbers. This is typically done with survey items where an excellent to poor = 1, 2, 3, 4 respectively.\n\n\n# Take a vector representing evaluation scores, named evaluate\nevaluate &lt;- c(\"excellent\", \"good\", \"fair\", \"poor\", \"excellent\", \"good\")\n# We can use a series of ifelse() commands to change the data to\n# numerical.\nevalNumerical &lt;- ifelse(evaluate == \"excellent\", 4, ifelse(evaluate ==\n    \"good\", 3, ifelse(evaluate == \"fair\", 2, 1)))\nevalNumerical\n\n[1] 4 3 2 1 4 3\n\n\n\n\n4.1.1.2 Nominal Variable\n\nWith nominal variables, data are simply categories for grouping.\nFor example, coding race/ethnicity might have a category value of White, Black, Native American, Asian/Pacific Islander, Other.\nQualitative values may be converted to quantitative values for analysis purposes. + White = 1, Black = 2, etc. This conversion to numerical representation of the category would be needed to run some analysis. + Sometimes, R does this on our behalf depending on commands used.\nWe can force a variable into a factor data type using the as.factor() command.\nIf we use the read.csv() command, we can sometimes do this by setting an argument \\(stringsAsFactors=TRUE\\). We will do this later in the lesson.\n\n\n\n\n4.1.2 Numerical data types:\n\nA Vector of Numbers (Real or Integer)\nContinuous (Real) variables can take any value along some continuum, hence continuous.\n\nWe can force a variable into a numerical data type by using the as.numeric() command.\nFor example, we could collect information on a participants age, height, weight, or distance traveled.\n\nTwo ways to create:\n\nWe can also create a numeric variable by ensuring our value we assign is a number!\nWe can force a variable into an real number data type by using the as.numeric() command.\n\n\n# Assign Rhode Island limit for medical marijuana in ounces per\n# person\nkOuncesRhode &lt;- 2.5\n# Identify the data type\nclass(x = kOuncesRhode)\n\n[1] \"numeric\"\n\n\nDiscrete (Integer) Variables:\n\nDiscrete variables can only take a countable number of distinct values.\nWe can force a variable into an integer data type by using the as.integer() command.\n\nFor example, we could collect information on the number of children in a family or number of points scored in a basketball game.\n\n\n# Assign the value of 4 to a constant called kTestInteger and set as\n# an integer\nkTestInteger &lt;- as.integer(4)\nclass(kTestInteger)  #Confirm the data type is an integer \n\n[1] \"integer\"\n\n# Use as.integer() to truncate the variable ouncesRhode\nTrunc &lt;- as.integer(kOuncesRhode)\nTrunc\n\n[1] 2\n\n\n\n\n4.1.3 Character data type\n\nWrapped in either single or double quotation marks.\n\nIncludes letters, words, or numbers that cannot logically be included in calculations (e.g., a zip code).\nA quick example is below that shows how to assign a character value to a variable.\n\n\n\n# Make constants\nkFirstName &lt;- \"Corina\"\nkLastName &lt;- \"Hughes\"\n# Check the data type\nclass(x = kFirstName)\n\n[1] \"character\"\n\n# Create a zip code constant and check the data type\nkZipCode &lt;- \"97405\"\nclass(x = kZipCode)\n\n[1] \"character\"\n\n\n\n\n4.1.4 Logical data type\n\nValues of TRUE and FALSE\nResult of some expression.\n\nA quick example is below that shows how to assign a logical value to a variable.\n\n\n# Store the result of 6 &gt; 8 in a constant called kSixEight\nkSixEight &lt;- 6 &gt; 8\n# Can use comparison tests with the following == &gt;= &lt;= &gt; &lt; &lt;&gt; !=\nkSixEight  # Print kSixEight\n\n[1] FALSE\n\n# Determine the data type of kSixEight\nclass(x = kSixEight)\n\n[1] \"logical\"\n\n\n\n\n\n4.1.5 Date data type\n\nA variable that should be a date.\n\n\n# Convert date info in format 'mm/dd/yyyy' using as.Date\nstrDates &lt;- c(\"01/05/1965\", \"08/16/1975\")\ndates &lt;- as.Date(strDates, \"%m/%d/%Y\")\nstr(dates)\n\n Date[1:2], format: \"1965-01-05\" \"1975-08-16\"\n\n\n\nlubridate is a package specifically for converting dates. This package makes dates a lot easier to work with.\n\n\n# Convert date info in format 'mm/dd/yyyy' using lubridate\nlibrary(lubridate)\nstrDates &lt;- c(\"01/05/1965\", \"08/16/1975\")\ndates &lt;- mdy(strDates)\nstr(dates)\n\n Date[1:2], format: \"1965-01-05\" \"1975-08-16\"\n\n\n\nIf you are only given a year and a month, you can use the ym() command to turn it to a date. But take note that it will add a day to the value as a placeholder.\n\n\n# Convert date info in format 'yyyymm' using lubridate\nstryyyymm &lt;- c(\"202201\", \"202003\", \"202204\")\ndates &lt;- ym(stryyyymm)\nstr(dates)\n\n Date[1:3], format: \"2022-01-01\" \"2020-03-01\" \"2022-04-01\"\n\n\n\n\n4.1.6 Nominal Example with Dataset\n\nlibrary(tidyverse)\ngss.2016 &lt;- read_csv(file = \"data/gss2016.csv\")\n\n\n# Examine the variable types with summary and class functions.\nsummary(gss.2016)\n\n    grass               age           \n Length:2867        Length:2867       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\nclass(gss.2016$grass)  #Check the data type.\n\n[1] \"character\"\n\ngss.2016$grass &lt;- as.factor(gss.2016$grass)  #Turn to a factor.\nclass(gss.2016$grass)  #Confirming it is now correct.\n\n[1] \"factor\"\n\n\n\n\n4.1.7 Numerical Example with Dataset\n\nWe need to ensure data can be coded as numeric before using the as.numeric() command. For example, to handle the variable age, it seems like numerical values except one value of “89 OR OLDER”. If as.numeric() command was used on this variable, it would put all the 89 and older observations as NAs. To force it to be a numerical variable, and keep that the sample participants were the oldest value, we need to recode it and then use the as.numeric() command to coerce it into a number.\nRecoding the 89 and older to 89 does cause the data to lack integrity in its current form because it will treat the people over 89 years old as 89. But, we are limited here because this needs to be a numerical variable for us to proceed. We will learn a step later on in this section to transform the age variable into categories so that we bring back our data integrity.\n\n\nclass(gss.2016$age)\n\n[1] \"character\"\n\n# Recode '89 OR OLDER' into just '89'\ngss.2016$age &lt;- recode(gss.2016$age, `89 OR OLDER` = \"89\")\n# Convert to numeric data type\ngss.2016$age &lt;- as.numeric(gss.2016$age)\nsummary(gss.2016)  #Conduct final check confirming correct data types\n\n       grass           age       \n DK       : 110   Min.   :18.00  \n IAP      : 911   1st Qu.:34.00  \n LEGAL    :1126   Median :49.00  \n NOT LEGAL: 717   Mean   :49.16  \n NA's     :   3   3rd Qu.:62.00  \n                  Max.   :89.00  \n                  NA's   :10",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#common-dplyr-functions",
    "href": "dataprep.html#common-dplyr-functions",
    "title": "4  Data Preparation",
    "section": "4.2 Common dplyr Functions",
    "text": "4.2 Common dplyr Functions\n\n4.2.1 Arrange\n\nSorting or arranging the dataset allows you to specify an order based on variable values.\nSorting allows us to review the range of values for each variable, and we can sort based on a single or multiple variables.\nNotice the difference between sort() and arrange() functions below.\n\nThe sort() function sorts a vector.\nThe arrange() function sorts a dataset based on a variable.\n\nTo conduct an example, read in the data set called gig.csv from your working directory.\n\n\ngig &lt;- read.csv(\"data/gig.csv\", stringsAsFactors = TRUE, na.strings = \"\")\ndim(gig)\n\n[1] 604   4\n\nhead(gig)\n\n  EmployeeID  Wage     Industry        Job\n1          1 32.81 Construction    Analyst\n2          2 46.00   Automotive   Engineer\n3          3 43.13 Construction  Sales Rep\n4          4 48.09   Automotive      Other\n5          5 43.62   Automotive Accountant\n6          6 46.98 Construction   Engineer\n\n\n\nUsing the arrange() function, we add the dataset, followed by a comma and then add in the variable we want to sort. This arranges from small to large.\nBelow is code to rearrange data based on Wage and save it in a new object.\n\n\nsortTidy &lt;- arrange(gig, Wage)\nhead(sortTidy)\n\n  EmployeeID  Wage     Industry        Job\n1        467 24.28 Construction   Engineer\n2        547 24.28 Construction  Sales Rep\n3        580 24.28 Construction Accountant\n4        559 24.42 Construction   Engineer\n5         16 24.76   Automotive Programmer\n6        221 24.76   Automotive Programmer\n\n\n\nWe can apply a desc() function inside the arrange function to re-sort from high to low like shown below.\n\n\nsortTidyDesc &lt;- arrange(gig, desc(Wage))\nhead(sortTidyDesc)\n\n  EmployeeID  Wage     Industry        Job\n1        110 51.00 Construction      Other\n2         79 50.00   Automotive   Engineer\n3        348 49.91 Construction Accountant\n4        373 49.91 Construction Accountant\n5        599 49.84   Automotive   Engineer\n6         70 49.77 Construction Accountant\n\n\n\n\n4.2.2 Subsetting or Filtering\n\nSubsetting or filtering a data frame is the process of indexing, or extracting a portion of the data set that is relevant for subsequent statistical analysis.\nYou can also use subset() or filter() commands as part of tidyverse.\nWe use subsets to do the following:\n\nView data based on specific data values or ranges.\nCompare two or more subsets of the data.\nEliminate observations that contain missing values, low-quality data, or outliers.\nExclude variables that contain redundant information, or variables with excessive amounts of missing values.\n\nWe can use the same technique as matrices to subset out particular rows and columns.\nLet’s do an example using the customers.csv file we read in earlier as customers in the last lesson.\nBase R provides several methods for subsetting data structures. Below uses base R by using the square brackets dataset[row, column] format.\n\n\ncustomers &lt;- read.csv(\"data/customers.csv\", stringsAsFactors = TRUE)\n\n# To subset, note the dataset[row,column] format Results hidden to\n# save space, but be sure to try this code in your .R file.  Data in\n# 1st row\ncustomers[1, ]\n# Data in 2nd column\ncustomers[, 2]\n# Data for 2nd column/1st observation (row)\ncustomers[1, 2]\n# First 3 columns of data\ncustomers[, 1:3]\n\n\nTidyverse is extremely popular when filtering data.\nThe filter function is used to subset rows of a data frame based on certain conditions.\nThe below example filters data by the College variable when category values are “Yes” and saves the filtered dataset into an object called college.\n\n\n# Filtering by whether the customer has a 'Yes' for college.  Saving\n# this filter into a new object college which you should see in your\n# global environment.\ncollege &lt;- filter(customers, College == \"Yes\")\n# Showing first 6 records of college - note the College variable is\n# all Yes's.\nhead(college)\n\n   CustID    Sex     Race  BirthDate College HHSize Income Spending Orders\n1 1530016 Female    Black 12/16/1986     Yes      5  53000      241      3\n2 1531136   Male    White   5/9/1993     Yes      5  94000      843     12\n3 1532160   Male    Black  5/22/1966     Yes      2  64000      719      9\n4 1532307   Male    White  9/16/1964     Yes      4  60000      582     13\n5 1532387   Male    White  8/27/1957     Yes      2  67000      452      9\n6 1533017 Female Hispanic  5/14/1985     Yes      3  84000      153      2\n  Channel\n1      SM\n2      TV\n3      TV\n4      SM\n5      SM\n6     Web\n\n\n\nUsing the filter command, we can add filters pretty easily by using an & for and, or an | for or. The statement below filters by College and Income and save the new dataset in an object called twoFilters.\n\n\ntwoFilters &lt;- filter(customers, College == \"Yes\" & Income &lt; 50000)\nhead(twoFilters)\n\n   CustID    Sex     Race  BirthDate College HHSize Income Spending Orders\n1 1533697 Female    Asian  10/8/1974     Yes      3  42000      247      3\n2 1535063 Female    White 12/17/1982     Yes      3  42000      313      4\n3 1544417   Male Hispanic  3/14/1980     Yes      4  46000      369      3\n4 1547864 Female Hispanic  6/15/1987     Yes      2  44000      500      5\n5 1550969 Female    White   4/8/1978     Yes      4  47000      774     16\n6 1553660 Female    White   8/2/1988     Yes      2  47000      745      5\n  Channel\n1     Web\n2      TV\n3      TV\n4      TV\n5      TV\n6      SM\n\n\n\nNext, we can do an or statement. The example below uses the filter command to filter by more than one category in the same field using the | in between the categories.\n\n\nTwoRaces &lt;- filter(customers, Race == \"Black\" | Race == \"White\")\nhead(TwoRaces)\n\n   CustID    Sex  Race  BirthDate College HHSize Income Spending Orders Channel\n1 1530016 Female Black 12/16/1986     Yes      5  53000      241      3      SM\n2 1531136   Male White   5/9/1993     Yes      5  94000      843     12      TV\n3 1532160   Male Black  5/22/1966     Yes      2  64000      719      9      TV\n4 1532307   Male White  9/16/1964     Yes      4  60000      582     13      SM\n5 1532387   Male White  8/27/1957     Yes      2  67000      452      9      SM\n6 1533791   Male White 10/27/1999     Yes      1  97000     1028     17     Web\n\n\n\n\n4.2.3 Select\n\nIn R, the select() function is part of the dplyr package, which is used for data manipulation. The select() function is specifically designed to subset or choose specific columns from a data frame. It allows you to select variables (columns) by their names or indices.\nBoth statements below select Income, Spending, and Orders variables from the customers dataset and form them into a new dataset called smallData.\nThe statements are written with and without the chaining operator.\n\n\nsmallData &lt;- select(customers, Income, Spending, Orders)\nhead(smallData)\n\n  Income Spending Orders\n1  53000      241      3\n2  94000      843     12\n3  64000      719      9\n4  60000      582     13\n5  47000      845      7\n6  67000      452      9\n\n\n\n\n4.2.4 Piping (Chaining) Operator\n\nThe pipe operator takes the output of the expression on its left-hand side and passes it as the first argument to the function on its right-hand side. This enables you to chain multiple functions together, making the code easier to understand and debug.\nIf we want to keep our code tidy, we can add the piping operator (%&gt;%) to help combine our lines of code into a new object or overwriting the same object.\nThis operator allows us to pass the result of one function/argument to the other one in sequence.\nThe below example uses a select function to pull Income, Spending, and Orders variables fromt he customers dataset and save it as a new object called smallData. It is an identical request to the one directly above, but written with the piping operator.\n\n\nsmallData &lt;- customers %&gt;%\n    select(Income, Spending, Orders)\n\n\n\n4.2.5 Counting\n\nCounting allows us to gain a better understanding and insights into the data.\nThis helps to verify that the data set is complete or determine if there are missing values.\nIn R, the length() function returns the number of elements in a vector, list, or any other object with a length attribute. It essentially counts the number of elements in the specified object.\n\n\n# Gives the length of Industry\nlength(gig$Industry)\n\n[1] 604\n\n\n\nFor counting using tidyverse, we typically use the filter and count function together to filter by a value or state and then count the filtered data.\nIn the function below, I use the piping operator to link together the filter and count functions into one command.\nNote that we need a piping operator (%&gt;%) before each new function that is part of the chunk.\n\n\n# Counting with a Categorical Variable Here we are filtering by\n# Automotive Industry and then counting the number and saving it in a\n# new object called countAuto\ncountAuto &lt;- gig %&gt;%\n    filter(Industry == \"Automotive\") %&gt;%\n    count(Industry)\ncountAuto  #190\n\n    Industry   n\n1 Automotive 190\n\n\n\nThe pull() function extracts a single column from a data frame as a vector.\n\n\n# Counting with a Numerical Variable We could also save this in an\n# object.\ngig %&gt;%\n    filter(Wage &gt; 30) %&gt;%\n    pull(Wage) %&gt;%\n    length()  ##536\n\n[1] 536\n\n\n\nWe learned that there are 190 employees in the automotive industry and there are 536 employees who earn more than $30 per hour.\nWe could also calculate the number of people with wages under or equal to 30.\n\n\n# We find 68 Wages under or equal to 30\nWageLess30 &lt;- gig %&gt;%\n    filter(Wage &lt;= 30) %&gt;%\n    pull(Wage) %&gt;%\n    length()  #\nWageLess30\n\n[1] 68\n\n\n\nYou try to find how many Accountants are in the Job Category of the gig data set. The answer is shown below.\n\n\n\n         Job  n\n1 Accountant 83\n\n\n\n\n4.2.6 Handling Missing Data\n\nAfter a data set is loaded, there are two common strategies for dealing with missing values.\n\n\nThe omission strategy recommends that observations with missing values be excluded from subsequent analysis.\nThe imputation strategy recommends that the missing values be replaced with some reasonable imputed values.\n\nNumeric variables: replace with the average.\nCategorical variables: replace with the predominant category.\n\n\n\n4.2.6.1 Limitations of Using a Missing Data Technique\n\nRecommended Closer Evaluation of Missing Data\nThere are limitations of both techniques listed above (omission and imputation).\n\nIf a large number of values are missing, mean imputation will likely distort the relationships among variables, leading to biased results.\nRemoving missing values could also significantly reduce your data set size.\nMissing data needs to be closely evaluated and verified within each variable whether the data is truly blank, has no answer, or is marked with a character value such as the text N/A.\nIf the variable that has many missing values is deemed unimportant or can be represented using a proxy variable that does not have missing values, the variable may be excluded from the analysis.\n\nMissing data needs to be closely evaluated to see if the missing value is meaningful or not.\n\nFor instance, getting data on how many pregnancies would only be applicable to people born of women gender, and blank value for people born of male gender, who are unable to have children, would be expected. In taking this example further, if variable 1 targeted the question, “how many pregnancies have you have had,” we would expect missing data or NAs for all the men. If comparing that variable to a second variable “Incubated from COVID-19: Yes/No” we would not want to omit all the blanks in the dataset because then we would eliminate analysis of an entire gender. Thus a different technique should be chosen besides omitting the blanks to be able to evaluate more concisely.\n\n\nIf a value is not blank and is considered missing, data needs to be mutated to be consistent with the technique of coding true missing values.\n\n\n\n4.2.6.2 The na.rm Parameter\n\ny &lt;- c(1, 2, NA, 3, 4, NA)\n# These lines runs, but do not give you anything useful.\nsum(y)\n\n[1] NA\n\nmean(y)\n\n[1] NA\n\n\n\nMany functions in R include parameters that will ignore NAs for you.\n\nsum() and mean() are examples of this, and most summary statistics like median() and var() also use the na.rm parameter to ignore the NAs. Always check the help to determine if na.rm is a parameter.\n\n\nsum(y, na.rm = TRUE)\n\n[1] 10\n\nmean(y, na.rm = TRUE)\n\n[1] 2.5\n\n# na.omit removes the NAs from the data set.\ny &lt;- na.omit(y)\n\n\n\n\n4.2.6.3 is.na()\n\nIn R, the is.na() function is used to check for missing (NA) values in objects like vectors, data frames, or arrays. It returns a logical vector of the same length as the input object, where TRUE indicates a missing value and FALSE indicates a non-missing value.\n\n\n# Gives the observation number of the observations that include NA\n# values\nwhich(is.na(gig$Industry))\n\n [1]  24 139 361 378 441 446 479 500 531 565\n\n# Produces a dataset with observations that have NA values in the\n# Industry field.\nShowBlankObservations &lt;- gig %&gt;%\n    filter(is.na(Industry))\nShowBlankObservations\n\n   EmployeeID  Wage Industry        Job\n1          24 42.58     &lt;NA&gt;  Sales Rep\n2         139 42.18     &lt;NA&gt;   Engineer\n3         361 31.33     &lt;NA&gt;      Other\n4         378 48.09     &lt;NA&gt;      Other\n5         441 32.35     &lt;NA&gt; Accountant\n6         446 30.76     &lt;NA&gt; Accountant\n7         479 42.85     &lt;NA&gt; Consultant\n8         500 43.13     &lt;NA&gt;  Sales Rep\n9         531 43.13     &lt;NA&gt;   Engineer\n10        565 38.98     &lt;NA&gt; Accountant\n\n# Counts the number of observations that have NA values in the\n# Industry field.\nCountBlanks &lt;- gig %&gt;%\n    filter(is.na(Industry)) %&gt;%\n    count(Industry)\nCountBlanks\n\n  Industry  n\n1     &lt;NA&gt; 10\n\n\n\n\n4.2.6.4 Using na_if()\n\nThe na_if() function in tidyr is used to replace specific values in a column with NA (missing) values. This function can be particularly useful when you want to standardize missing values across a dataset or when you want to replace certain values with NA for further data processing\n\n\nTurnNA &lt;- gig %&gt;%\n    mutate(Job = na_if(Job, \"Other\"))\nhead(TurnNA)\n\n  EmployeeID  Wage     Industry        Job\n1          1 32.81 Construction    Analyst\n2          2 46.00   Automotive   Engineer\n3          3 43.13 Construction  Sales Rep\n4          4 48.09   Automotive       &lt;NA&gt;\n5          5 43.62   Automotive Accountant\n6          6 46.98 Construction   Engineer\n\n\n\n\n4.2.6.5 na.omit() vs. drop_na()\n\nBoth functions return a new object with the rows containing missing values removed.\nna.omit() is a base R function, so it doesn’t require any additional package installation where drop_na() requires loading the tidyr package, which is part of the tidyverse ecosystem.\ndrop_na() fits well into tidyverse pipelines, making it easy to integrate with other tidyverse functions where na.omit() can also be used in pipelines but might require additional steps to fit seamlessly.\n\n\n# install.packages('Amelia')\nlibrary(Amelia)\ndata(\"africa\")\nsummary(africa)\n\n      year              country       gdp_pc            infl        \n Min.   :1972   Burkina Faso:20   Min.   : 376.0   Min.   : -8.400  \n 1st Qu.:1977   Burundi     :20   1st Qu.: 513.8   1st Qu.:  4.760  \n Median :1982   Cameroon    :20   Median :1035.5   Median :  8.725  \n Mean   :1982   Congo       :20   Mean   :1058.4   Mean   : 12.753  \n 3rd Qu.:1986   Senegal     :20   3rd Qu.:1244.8   3rd Qu.: 13.560  \n Max.   :1991   Zambia      :20   Max.   :2723.0   Max.   :127.890  \n                                  NA's   :2                         \n     trade            civlib         population      \n Min.   : 24.35   Min.   :0.0000   Min.   : 1332490  \n 1st Qu.: 38.52   1st Qu.:0.1667   1st Qu.: 4332190  \n Median : 59.59   Median :0.1667   Median : 5853565  \n Mean   : 62.60   Mean   :0.2889   Mean   : 5765594  \n 3rd Qu.: 81.16   3rd Qu.:0.3333   3rd Qu.: 7355000  \n Max.   :134.11   Max.   :0.6667   Max.   :11825390  \n NA's   :5                                           \n\nsummary(africa$gdp_pc)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  376.0   513.8  1035.5  1058.4  1244.8  2723.0       2 \n\nsummary(africa$trade)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  24.35   38.52   59.59   62.60   81.16  134.11       5 \n\nafrica1 &lt;- na.omit(africa)\nsummary(africa1)\n\n      year              country       gdp_pc            infl       \n Min.   :1972   Burkina Faso:20   Min.   : 376.0   Min.   : -8.40  \n 1st Qu.:1976   Burundi     :17   1st Qu.: 511.5   1st Qu.:  4.67  \n Median :1981   Cameroon    :18   Median :1062.0   Median :  8.72  \n Mean   :1981   Congo       :20   Mean   :1071.8   Mean   : 12.91  \n 3rd Qu.:1986   Senegal     :20   3rd Qu.:1266.0   3rd Qu.: 13.57  \n Max.   :1991   Zambia      :20   Max.   :2723.0   Max.   :127.89  \n     trade            civlib         population      \n Min.   : 24.35   Min.   :0.0000   Min.   : 1332490  \n 1st Qu.: 38.52   1st Qu.:0.1667   1st Qu.: 4186485  \n Median : 59.59   Median :0.1667   Median : 5858750  \n Mean   : 62.60   Mean   :0.2899   Mean   : 5749761  \n 3rd Qu.: 81.16   3rd Qu.:0.3333   3rd Qu.: 7383000  \n Max.   :134.11   Max.   :0.6667   Max.   :11825390  \n\n## to drop all at once.\nafrica2 &lt;- africa %&gt;%\n    drop_na()\nsummary(africa2)\n\n      year              country       gdp_pc            infl       \n Min.   :1972   Burkina Faso:20   Min.   : 376.0   Min.   : -8.40  \n 1st Qu.:1976   Burundi     :17   1st Qu.: 511.5   1st Qu.:  4.67  \n Median :1981   Cameroon    :18   Median :1062.0   Median :  8.72  \n Mean   :1981   Congo       :20   Mean   :1071.8   Mean   : 12.91  \n 3rd Qu.:1986   Senegal     :20   3rd Qu.:1266.0   3rd Qu.: 13.57  \n Max.   :1991   Zambia      :20   Max.   :2723.0   Max.   :127.89  \n     trade            civlib         population      \n Min.   : 24.35   Min.   :0.0000   Min.   : 1332490  \n 1st Qu.: 38.52   1st Qu.:0.1667   1st Qu.: 4186485  \n Median : 59.59   Median :0.1667   Median : 5858750  \n Mean   : 62.60   Mean   :0.2899   Mean   : 5749761  \n 3rd Qu.: 81.16   3rd Qu.:0.3333   3rd Qu.: 7383000  \n Max.   :134.11   Max.   :0.6667   Max.   :11825390  \n\n\n\nYou try to load the airquality dataset from base R and look at a summary of the dataset.\n\nSum the number of NAs in airquality.\nOmit all the NAs from airquality and save it in a new data object called airqual and take a new summary of it.\n\n\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n\n\n\n[1] 44\n\n\n     Ozone          Solar.R           Wind            Temp      \n Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n 1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n 3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n     Month            Day       \n Min.   :5.000   Min.   : 1.00  \n 1st Qu.:6.000   1st Qu.: 9.00  \n Median :7.000   Median :16.00  \n Mean   :7.216   Mean   :15.95  \n 3rd Qu.:9.000   3rd Qu.:22.50  \n Max.   :9.000   Max.   :31.00  \n\n\n\n\n\n\n4.2.7 Summarize\n\nThe summarize() command is used to create summary statistics for groups of observations in a data frame.\nIn the example below, we can summarize more than one thing into tidy output.\n\n\ngig %&gt;%\n    drop_na() %&gt;%\n    summarize(mean.days = mean(Wage), sd.days = sd(Wage), var.days = var(Wage),\n        med.days = median(Wage), iqr.days = IQR(Wage))\n\n  mean.days  sd.days var.days med.days iqr.days\n1  40.14567 7.047058 49.66103    41.82   11.465\n\n\n\n\n4.2.8 Group_by\n\ngroup_by is used for grouping data by one or more variables. When you use group_by() on a data frame, it doesn’t actually perform any computations immediately. Instead, it sets up the data frame in such a way that any subsequent operations are performed within these groups\nsummarize() is often used in combination with group_by() to calculate summary statistics within groups\n\n\n## summarize data by Industry variable.\ngroupedData &lt;- gig %&gt;%\n    group_by(Industry) %&gt;%\n    summarize(meanWage = mean(Wage))\ngroupedData\n\n# A tibble: 4 × 2\n  Industry     meanWage\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Automotive       43.4\n2 Construction     38.3\n3 Tech             40.7\n4 &lt;NA&gt;             39.5\n\n## same function with na's dropped.\ngroupedData &lt;- gig %&gt;%\n    drop_na() %&gt;%\n    group_by(Industry) %&gt;%\n    summarize(meanWage = mean(Wage))\ngroupedData\n\n# A tibble: 3 × 2\n  Industry     meanWage\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Automotive       43.4\n2 Construction     38.4\n3 Tech             40.7\n\n\n\n\n4.2.9 Mutate\n\nmutate() is part of the dplyr package, which is used for data manipulation. The mutate() function is specifically designed to create new variables (columns) or modify existing variables in a data frame. It is commonly used in data wrangling tasks to add calculated columns or transform existing ones.\nOne example is below, but note that there are many things you can do with the mutate function.\n\n\n# making a new variable called calculation that multiplies gdp_pc by\n# infl variables in the africa1 dataset.\nafrica.mutated &lt;- mutate(africa1, calculation = gdp_pc * infl)\nhead(africa.mutated)\n\n  year      country gdp_pc  infl trade    civlib population calculation\n1 1972 Burkina Faso    377 -2.92 29.69 0.5000000    5848380    -1100.84\n2 1973 Burkina Faso    376  7.60 31.31 0.5000000    5958700     2857.60\n3 1974 Burkina Faso    393  8.72 35.22 0.3333333    6075700     3426.96\n4 1975 Burkina Faso    416 18.76 40.11 0.3333333    6202000     7804.16\n5 1976 Burkina Faso    435 -8.40 37.76 0.5000000    6341030    -3654.00\n6 1977 Burkina Faso    448 29.99 41.11 0.6666667    6486870    13435.52\n\n\n\ndata(\"iris\")\n## Selecting 2 variables from the iris dataset: Sepal.Length and\n## Petal.Length\nselected_data &lt;- select(iris, Sepal.Length, Petal.Length)\nhead(selected_data)\n\n  Sepal.Length Petal.Length\n1          5.1          1.4\n2          4.9          1.4\n3          4.7          1.3\n4          4.6          1.5\n5          5.0          1.4\n6          5.4          1.7\n\n# Filter rows based on a condition: Species = setosa\nfiltered_data &lt;- filter(iris, Species == \"setosa\")\nhead(filtered_data)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# Arrange rows by the Sepal.Length column\narranged_data &lt;- arrange(iris, Sepal.Length)\n# Create a new column by mutating the data by transforming\n# Petal.Width to the log form.\nmutated_data &lt;- mutate(iris, Petal.Width_Log = log(Petal.Width))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#full-examples",
    "href": "dataprep.html#full-examples",
    "title": "4  Data Preparation",
    "section": "4.3 Full Examples",
    "text": "4.3 Full Examples\n\n4.3.1 gss.2016 Data Cleaning\n\nFirst, because we made some edits to the data set, reread in version a using the read.csv command. This brings the data set back to its original form. It is always a good idea to read the dataset back in when you are unsure about whether you have made a mistake during data preparation that could cause a lack of data integrity.\n\n\n\ngss.2016 &lt;- read.csv(file = \"data/gss2016.csv\")\n\n\nBefore we remove any missing data, we need it to be the correct data type. In this case, grass should be a factor.\n\n\n# We coerced this variable earlier, but the object was called\n# gss.2016.  Since we reread in the data set, this needs to be done\n# again.\ngss.2016$grass &lt;- as.factor(gss.2016$grass)\n\n\nThe statement below is an equivalent to the function above, but written with the piping operator. It is overwriting gss.2016 after conducting the coercion to factor.\nWe added the mutate function because we are going to add other data cleaning tasks to this statement.\n\n\ngss.2016 &lt;- gss.2016 %&gt;%\n    mutate(grass = as.factor(grass))\n\n\n4.3.1.1 Piping to More Functions: Missing Data\n\nIn the code below, the as.factor() command has been moved inside a broader mutate statement (that uses tidyverse library) and piped to it the na_if() command that handles missing data. If you use more than one data manipulation statement, the mutate() command is needed to help organize your code with one mutate() is needed for each major change you are making.\nIn the code below, we created a new object gss.2016.cleaned to help store the cleaned version of the dataset. This helps maintain data integrity because your original dataset is still intact and each time, you rerun the entire chunk, which includes all the changes at one time.\n\n\ngss.2016.cleaned &lt;- gss.2016 %&gt;%\n    # Moved coercion statement into a mutate function to keep code\n    # tidy\nmutate(grass = as.factor(grass)) %&gt;%\n    # Moving DK value to NA for not applicable\nmutate(grass = na_if(x = grass, y = \"DK\"))\n\n# Check the summary, there should be 110 + 3 = 113 in the NA category\nsummary(object = gss.2016.cleaned)\n\n       grass          age           \n DK       :   0   Length:2867       \n IAP      : 911   Class :character  \n LEGAL    :1126   Mode  :character  \n NOT LEGAL: 717                     \n NA's     : 113                     \n\n\n\n\n4.3.1.2 Drop Levels\n\nThe droplevels function is part of base R and is used to drop unused levels from factor variables in a data frame. It works by removing any levels from a factor variable that are not present in the data.\nNext, we want to edit our code to convert IAP and DK to NA values and drop levels that have are empty.\n\nNote the Piping operator added to the end of the DK line so you can keep going with new commands editing gss.2016.cleaned.\n\n\ngss.2016.cleaned &lt;- gss.2016 %&gt;%\n    mutate(grass = as.factor(grass)) %&gt;%\n    # Added piping operator\nmutate(grass = na_if(x = grass, y = \"DK\")) %&gt;%\n    # Turn to na if value of grass = IAP\nmutate(grass = na_if(x = grass, y = \"IAP\")) %&gt;%\n    # Drop levels in grass that have no values\nmutate(grass = droplevels(x = grass))\n# Check what you just did\nsummary(gss.2016.cleaned)\n\n       grass          age           \n LEGAL    :1126   Length:2867       \n NOT LEGAL: 717   Class :character  \n NA's     :1024   Mode  :character  \n\n\n\n\n\n4.3.1.3 Coercing to Numeric\n\nNext, we handle a numerical variable, age. Age again has an issue being able to be numerical data type because it has “89 OR OLDER” as a value. Before using the as.numeric() command, we need to recode it. We did this above as a stand-alone statement.\n\n\n\ngss.2016.cleaned &lt;- gss.2016 %&gt;%\n    mutate(grass = as.factor(grass)) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"DK\")) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"IAP\")) %&gt;%\n    # Added piping operator\nmutate(grass = droplevels(x = grass)) %&gt;%\n    # Ensure variable can be coded as numeric and fix if necessary.\nmutate(age = recode(age, `89 OR OLDER` = \"89\")) %&gt;%\n    # Coerce into numeric\nmutate(age = as.numeric(x = age))\n\n# Check what you just did\nsummary(gss.2016.cleaned)\n\n       grass           age       \n LEGAL    :1126   Min.   :18.00  \n NOT LEGAL: 717   1st Qu.:34.00  \n NA's     :1024   Median :49.00  \n                  Mean   :49.16  \n                  3rd Qu.:62.00  \n                  Max.   :89.00  \n                  NA's   :10     \n\n\n\nThe recode() command that is part of dplyr is like the ifelse() command that is in base R. There are a lot of ways to recode in R.\nFinally, we want to take our numerical variable, age, and cut it at certain breaks to make categories that can be easily analyzed.\n\nThis also ensures that anyone above 89 is coded correctly in a category instead of as the value 89. This again brings back data integrity.\nThe cut() function generates class limits and bins used in frequency distributions (and histograms) for quantitative data.\nHere, we are using it to cut age into a categorical variable.\n\n\ngss.2016.cleaned &lt;- gss.2016 %&gt;%\n    mutate(grass = as.factor(grass)) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"DK\")) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"IAP\")) %&gt;%\n    mutate(grass = droplevels(grass)) %&gt;%\n    mutate(age = recode(age, `89 OR OLDER` = \"89\")) %&gt;%\n    # Added piping operator\nmutate(age = as.numeric(age)) %&gt;%\n    # Cut numeric variable into groupings\nmutate(age.cat = cut(age, breaks = c(-Inf, 29, 59, 74, Inf), labels = c(\"&lt; 30\",\n    \"30 - 59\", \"60 - 74\", \"75+\")))\n\n# Check what you just did\nsummary(gss.2016.cleaned)\n\n       grass           age           age.cat    \n LEGAL    :1126   Min.   :18.00   &lt; 30   : 481  \n NOT LEGAL: 717   1st Qu.:34.00   30 - 59:1517  \n NA's     :1024   Median :49.00   60 - 74: 598  \n                  Mean   :49.16   75+    : 261  \n                  3rd Qu.:62.00   NA's   :  10  \n                  Max.   :89.00                 \n                  NA's   :10                    \n\n\n\n\n\n\n4.3.2 brfss Data Cleaning\n\nThe full codebook where this screenshot is taken is brfss_2014_codebook.pdf.\n\n\n\n\nEvaluate CodeBook Before Making Decisions\n\n\n\nbrfss &lt;- read.csv(\"data/brfss.csv\")\nsummary(brfss)\n\n    TRNSGNDR        X_AGEG5YR          X_RACE         X_INCOMG    \n Min.   :1.00     Min.   : 1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.00     1st Qu.: 5.000   1st Qu.:1.000   1st Qu.:3.000  \n Median :4.00     Median : 8.000   Median :1.000   Median :5.000  \n Mean   :4.06     Mean   : 7.822   Mean   :1.992   Mean   :4.481  \n 3rd Qu.:4.00     3rd Qu.:10.000   3rd Qu.:1.000   3rd Qu.:5.000  \n Max.   :9.00     Max.   :14.000   Max.   :9.000   Max.   :9.000  \n NA's   :310602                    NA's   :94                     \n    X_EDUCAG        HLTHPLN1         HADMAM          X_AGE80     \n Min.   :1.000   Min.   :1.000   Min.   :1.00     Min.   :18.00  \n 1st Qu.:2.000   1st Qu.:1.000   1st Qu.:1.00     1st Qu.:44.00  \n Median :3.000   Median :1.000   Median :1.00     Median :58.00  \n Mean   :2.966   Mean   :1.108   Mean   :1.22     Mean   :55.49  \n 3rd Qu.:4.000   3rd Qu.:1.000   3rd Qu.:1.00     3rd Qu.:69.00  \n Max.   :9.000   Max.   :9.000   Max.   :9.00     Max.   :80.00  \n                                 NA's   :208322                  \n    PHYSHLTH   \n Min.   : 1.0  \n 1st Qu.:20.0  \n Median :88.0  \n Mean   :61.2  \n 3rd Qu.:88.0  \n Max.   :99.0  \n NA's   :4     \n\n\n\n4.3.2.1 Qualitative Variable\n\nTo look at an example, the one below seeks to understand the healthcare issue in reporting gender based on different definitions. The dataset is part of the Behavioral Risk Factor Surveillance System (brfss) dataset (2014), which includes lots of other variables besides reported gender.\n\n\n# Load the data\nbrfss &lt;- read.csv(\"data/brfss.csv\")\n# Summarize the TRNSGNDR variable\nsummary(object = brfss$TRNSGNDR)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00    4.00    4.00    4.06    4.00    9.00  310602 \n\n# Find frequencies\ntable(brfss$TRNSGNDR)\n\n\n     1      2      3      4      7      9 \n   363    212    116 150765   1138   1468 \n\n\n\nSince this table is not very informative, we need to do some edits.\nCheck the class of the variable to see the issue with analyzing it as a categorical variable.\n\n\nclass(brfss$TRNSGNDR)\n\n[1] \"integer\"\n\n\n\nFirst, we need to change the TRNSGNDR variable to a factor using as.factor().\n\n\n# Change variable from numeric to factor\nbrfss$TRNSGNDR &lt;- as.factor(brfss$TRNSGNDR)\n# Check data type again to ensure factor\nclass(brfss$TRNSGNDR)\n\n[1] \"factor\"\n\n\n\nThen, we need to do some data cleaning on the TRNSGNDR Variable.\n\n\nbrfss.cleaned &lt;- brfss %&gt;% \n  mutate(TRNSGNDR = recode_factor(TRNSGNDR,\n      '1' = 'Male to female',\n      '2' = 'Female to male',\n      '3' = 'Gender non-conforming',\n      '4' = 'Not transgender',\n      '7' = 'Not sure',\n      '9' = 'Refused'))\n\n\nWe can use the levels() command to show the factor levels made with the mutate() command above.\n\n\nlevels(brfss.cleaned$TRNSGNDR)\n\n[1] \"Male to female\"        \"Female to male\"        \"Gender non-conforming\"\n[4] \"Not transgender\"       \"Not sure\"              \"Refused\"              \n\n\n\nCheck the summary.\n\n\nsummary(brfss.cleaned$TRNSGNDR)\n\n       Male to female        Female to male Gender non-conforming \n                  363                   212                   116 \n      Not transgender              Not sure               Refused \n               150765                  1138                  1468 \n                 NA's \n               310602 \n\n\n\nTake a good look at the table to interpret the frequencies in the output above. The highest percentage was the “NA’s” category, followed by “Not transgender”. Removing the NA’s moved the “Not transgender” category to over 97% of observations.\n\n\n\n4.3.2.2 Quantitative Variable\n\nLet’s use the cleaned dataset to make more changes to the continuous variable PHYSHLTH. In the codebook, it looks like the data is most applicable to the first 2 categories. The 1-30 days coding and the 88 coding, which means 0 days of physical illness and injury.\n\nUsing cleaned data, we need to prep the variable a little more before getting an accurate plot.\nSpecifically, we need to null out the 77 and 99 values and make sure the 88 coding is set to be 0 for 0 days of illness and injury.\n\n\n\nbrfss.cleaned &lt;- brfss %&gt;%\n    mutate(TRNSGNDR = recode_factor(TRNSGNDR, `1` = \"Male to female\", `2` = \"Female to male\",\n        `3` = \"Gender non-conforming\", `4` = \"Not transgender\", `7` = \"Not sure\",\n        `9` = \"Refused\")) %&gt;%\n    # Turn the 77 values to NA's.\nmutate(PHYSHLTH = na_if(PHYSHLTH, y = 77)) %&gt;%\n    # Turn the 99 values to NA's.\nmutate(PHYSHLTH = na_if(PHYSHLTH, y = 99)) %&gt;%\n    # Recode the 88 values to be numeric value of 0.\nmutate(PHYSHLTH = recode(PHYSHLTH, `88` = 0L))\n\n\nThe histogram showed most people have between 0 and 10 unhealthy days per 30 days.\nNext, evaluate mean, median, and mode for the PHYSHLTH variable after ignoring the blanks.\n\n\nmean(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 4.224106\n\nmedian(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 0\n\nnames(x = sort(x = table(brfss.cleaned$PHYSHLTH), decreasing = TRUE))[1]\n\n[1] \"0\"\n\n\n\nWhile the mean is higher at 4.22, the median and most common number is 0.\n\n\n## Spread to Report with the Mean\nvar(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 77.00419\n\nsd(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 8.775203\n\n## Spread to Report with Median\nsummary(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   0.000   0.000   4.224   3.000  30.000   10303 \n\nrange(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1]  0 30\n\nmax(brfss.cleaned$PHYSHLTH, na.rm = TRUE) - min(brfss.cleaned$PHYSHLTH,\n    na.rm = TRUE)\n\n[1] 30\n\nIQR(brfss.cleaned$PHYSHLTH, na.rm = TRUE)\n\n[1] 3\n\n\n\nlibrary(semTools)\n# Plot the data\nbrfss.cleaned %&gt;%\n    ggplot(aes(PHYSHLTH)) + geom_histogram()\n\n\n\n\n\n\n\n# Calculate Skewness and Kurtosis\nskew(brfss.cleaned$PHYSHLTH)\n\n   skew (g1)           se            z            p \n2.209078e+00 3.633918e-03 6.079054e+02 0.000000e+00 \n\nkurtosis(brfss.cleaned$PHYSHLTH)\n\nExcess Kur (g2)              se               z               p \n   3.474487e+00    7.267836e-03    4.780634e+02    0.000000e+00 \n\n\n\nThe skew results provide a z of 607.905 (6.079054e+02) which is much higher than 7 (for large datasets). This indicates a clear right skew which means the data is not normally distributed.\nThe kurtosis results are also very leptokurtic with a score of 478.063.\n\n\n\n4.3.2.3 Using Filters Example\n\nBelow takes an example of the brfss data to filter by certain variable statuses.\n\nThe first filter() chose observations that were any one of the three categories of transgender included in the data. Used the | “or” operator for this filter().\nThe second filter chose people in an age category above category 4 but below category 12, in the age categories 5 through 11.\nThe last filter used the !is.na to choose observations where HADMAM variable was not NA.\n\nNext, we reduce data set to contain only variables used to create table by using the select() command.\nNext, we change all the remaining variables in data set to factors using mutate_all() command. This not only changes the strings to factors, but also changes the numerical variables to factors.\nFinally, we use mutate() commands to change the variable category to something meaningful(from the codebook).\n\nNotice the backslash before the apostrophe in Don’t in the X_INCOMG recode. This is to prevent the .R file from ending the quotations. You could use double quotes around the statement to bypass this, or add the backslash like I did here.\n\n\nbrfss_small &lt;- brfss.cleaned %&gt;%\n    filter(TRNSGNDR == \"Male to female\" | TRNSGNDR == \"Female to male\" |\n        TRNSGNDR == \"Gender non-conforming\") %&gt;%\n    filter(X_AGEG5YR &gt; 4 & X_AGEG5YR &lt; 12) %&gt;%\n    filter(!is.na(HADMAM)) %&gt;%\n    select(TRNSGNDR, X_AGEG5YR, X_RACE, X_INCOMG, X_EDUCAG, HLTHPLN1, HADMAM) %&gt;%\n    mutate_all(as.factor) %&gt;%\n    # The next few mutates add labels to categorical variables based\n    # on the codebook.\nmutate(X_AGEG5YR = recode_factor(X_AGEG5YR, `5` = \"40-44\", `6` = \"45-49\",\n    `7` = \"50-54\", `8` = \"55-59\", `9` = \"60-64\", `10` = \"65-69\", `11` = \"70-74\")) %&gt;%\n    mutate(X_INCOMG = recode_factor(X_INCOMG, `1` = \"Less than 15,000\",\n        `2` = \"15,000 to less than 25,000\", `3` = \"25,000 to less than 35,000\",\n        `4` = \"35,000 to less than 50,000\", `5` = \"50,000 or more\", `9` = \"Don't know/not sure/missing\")) %&gt;%\n    mutate(X_EDUCAG = recode_factor(X_EDUCAG, `1` = \"Did not graduate high school\",\n        `2` = \"Graduated high school\", `3` = \"Attended college/technical school\",\n        `4` = \"Graduated from college/technical school\", `9` = NA_character_)) %&gt;%\n    mutate(HLTHPLN1 = recode_factor(HLTHPLN1, `1` = \"Yes\", `2` = \"No\",\n        `7` = \"Don't know/not sure/missing\", `9` = \"Refused\")) %&gt;%\n    mutate(X_RACE = recode_factor(X_RACE, `1` = \"White\", `2` = \"Black\",\n        `3` = \"Native American\", `4` = \"Asian/Pacific Islander\", `5` = \"Other\",\n        `6` = \"Other\", `7` = \"Other\", `8` = \"Other\", `9` = \"Other\"))\n# print a summary\nsummary(brfss_small)\n\n                  TRNSGNDR   X_AGEG5YR                     X_RACE   \n Male to female       : 77   40-44:27   White                 :152  \n Female to male       :113   45-49:27   Black                 : 31  \n Gender non-conforming: 32   50-54:32   Native American       :  4  \n Not transgender      :  0   55-59:44   Asian/Pacific Islander:  6  \n Not sure             :  0   60-64:44   Other                 : 29  \n Refused              :  0   65-69:24                               \n                             70-74:24                               \n                        X_INCOMG                                     X_EDUCAG \n Less than 15,000           :46   Did not graduate high school           :24  \n 15,000 to less than 25,000 :44   Graduated high school                  :86  \n 25,000 to less than 35,000 :19   Attended college/technical school      :68  \n 35,000 to less than 50,000 :26   Graduated from college/technical school:44  \n 50,000 or more             :65                                               \n Don't know/not sure/missing:22                                               \n\n HLTHPLN1  HADMAM \n Yes:198   1:198  \n No : 24   2: 22  \n           9:  2  \n\n\n\n\n\n\nThis data set full of categorical variables is now fully cleaned and ready to be analyzed!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataprep.html#summary",
    "href": "dataprep.html#summary",
    "title": "4  Data Preparation",
    "section": "4.4 Summary",
    "text": "4.4 Summary\n\nIn this lesson, we worked through the basics on data cleaning. Data cleaning is so important and there are so many ways to do it. Provided are some examples using popular functions in dplyr (under tidyverse).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Preparation</span>"
    ]
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.0.1 At a Glance",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#graphs-for-a-single-categorical-variable",
    "href": "dataviz.html#graphs-for-a-single-categorical-variable",
    "title": "5  Data Visualization",
    "section": "5.1 Graphs for a Single Categorical Variable",
    "text": "5.1 Graphs for a Single Categorical Variable\n\nA categorical variable has categories that are either ordinal with a logical order or nominal with no logical order.\nCategorical variables need to be set as the factor data type in R to be able to be analyzed and visualized correctly.\nSome common graphing options for single categorical variable:\n\nBar graph\nPie chart\n\nIn any graph, it is beneficial to do any data cleaning and investigation into the variable(s) before you begin. With categorical variables, this may require recoding the factor(s) of interest and possibly renaming it/them to something meaningful if needed.\n\n\n5.1.1 Bar Graph\n\nA bar graph depicts the frequency or the relative frequency for each category of the qualitative data as a bar rising vertically from the horizontal axis. A bar graph is also known as a bar chart and is often used to examine similarities and differences across categories of things; bars can represent frequencies, percentages, means, or other statistics.\nWe can learn a lot from a bar graph, like the marital status group with the highest and lowest frequencies according to the census.gov.\n\n\n\n\n\n\n\n\n\n\n\n5.1.1.1 geom_bar()\n\nCreate a Bar Graph using ggplot() Command\nLike histograms, ggplot has many more parameters available over base R to construct bar graphs.\nFirst, we load tidyerse to access ggplot() command and others. You can always do this at the start of all your code to keep all the libraries together that are being used.\n\n\nlibrary(\"tidyverse\")\n\n\nggplot() works in layers, so you will routinely see the + symbol to kick off a new layer with added functionality.\nUsing the ggplot, we always include the aes() command first inside the ggplot() command. The aes() command is a quoting function that describes the variables being used. From there, it depends on the plot.\n\nFirst layer: ggplot() and aes() which calls the dataset and variables used.\nSecond layer: Graph type: Bar graph: geom_bar().\nAdditional layers: labs - for labels including titles; themes; and geom_text. Recreate the example below adding one layer at a time to see how the visualization changes.\n\n\n\n## inputting probabilities calculated from a 2023 multiple choice\n## question.  From what you learned about R so far, how do you expect\n## its market share to change?\nGoUp &lt;- 0.54285\nGoDown &lt;- 0.03809\nRemainStable &lt;- 0.34285\nNoOpinion &lt;- 0.07619\n# designing the data frame\ndata_frame &lt;- data.frame(Category = c(\"Go Up\", \"Go Down\", \"Remain Stable\",\n    \"No Opinion\"), Percentage = c(GoUp, GoDown, RemainStable, NoOpinion))\n# Making the graph\nMarketShare &lt;- ggplot(data_frame, aes(x = Category, y = Percentage, fill = Category)) +\n    geom_bar(stat = \"identity\") + labs(title = \"How do you expect R's market share to change?\",\n    x = \"Opinion Category\", y = \"Percentage (%)\") + theme_minimal() + geom_text(aes(label = Percentage),\n    vjust = -0.5, size = 4)\nMarketShare\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Bar Graph with Data Wrangling\n\nWe can also use a dataset to make a bar graph.\n\nLoad the Gun related data from the nhanes survey to run an example.\n\n\nnhanes &lt;- read.csv(\"data/nhanes2012.csv\")\n\n\nNext, we need to check the import by looking at the summary or head of the data.\n\n\n# Results hidden to save space, but gives you the first 6 records in\n# the data set.\nhead(nhanes)\n\n\nWe can also check the summary of data of only the variable of interest, AUQ300, to get a sense of what we are evaluating.\n\n\nsummary(nhanes$AUQ300)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  1.000   1.000   2.000   1.656   2.000   7.000    4689 \n\n\n\nThe AUQ300 variable represents gun use. A screenshot of the codebook is copied below so that we can see what AUQ300 really refers to. It is available on https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/AUQ_G.htm. This is always a necessary step because variable names can be convoluted and not representative of the variable definition.\n\n\n\n\nAUQ300\n\n\n\n5.1.2.1 Recode Variable if Needed\n\nLook to see if the AUQ300 needs recoding after looking at the codebook and making sense of the variable.\nAUQ300 needs to be a factor variable with 1 equaling a Yes and 2 equaling a No. We can use recode_factor to accomplish 2 things at once with the mutate function.\nrecode_factor() transforms the levels of a categorical variable (factor) into a new set of levels and is specific to categorical variables.\nrecode() is generic and can apply to numerical, categorical, or textual data, but still transforms data from one format or code to another.\n\n\nnhanes.clean &lt;- nhanes %&gt;%\n    mutate(AUQ300 = recode_factor(AUQ300, `1` = \"Yes\", `2` = \"No\"))\n\n\nThen, we need to check the recode for accuracy. You should see the No’s and Yes’s alongside the rest being coded as NA’s.\n\n\nsummary(nhanes.clean$AUQ300)\n\n Yes   No NA's \n1613 3061 4690 \n\n\n\n\n5.1.2.2 Get Bar Roughly Plotted\n\nStart with the basic plot using the ggplot() and geom_bar() commands.\nBelow writes the statement with and without the piping operator.\n\nSince we are also going to use data preparation techniques, the piping operator is recommended.\n\n\n# Without piping operator\nggplot(nhanes.clean, aes(x = AUQ300)) + geom_bar()\n\n\n\n\n\n\n\n# With piping operator\nnhanes.clean %&gt;%\n    ggplot(aes(x = AUQ300)) + geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n5.1.2.3 Add Functions to Clean Chart\n\nOmit the NA category from AUQ300 variable, which represents gun use. Then plot the graph below.\nThe drop_na() function is a good way to drop NA values from either the entire dataset or just one variable. It was introduced in the data prep lesson. Since we are only interested in dropping NA values from our one variable of interest that is to be graphed (AUQ300), we can put it in the parentheses so that we do not unintentionally drop lots of observations for no reason.\nAdd an axis labels under labs(x = …, y=…).\n\n\nnhanes.clean %&gt;%\n  drop_na(AUQ300) %&gt;%\n  ggplot(aes(x = AUQ300)) + geom_bar() +\n  labs(x = \"Gun use\", y = \"Number of participants\")\n\n\n\n\n\n\n\n##Here, we really benefit from the piping operator because we are doing more than one thing.\n\n\nFrom the bar graph, we can see that almost double the amount of people have not fired a firearm for any reason than those that fired one.\n\n\n\n5.1.2.4 Adding Color\n\nThere are many ways to add color to a bar graph. Below, the color is filled in directly in the aes() command by choosing it to give a different color to each categorical value of AUQ300.\n\nWhen fill is mapped to a variable, the fill color of the geom will vary based on the values of that variable. This is useful for distinguishing different groups or categories within the data. In this case the fill=AUQ300 gives a distinct color pattern based on how many categories there are considering the fact we are using the default “scale.”\n\n\nnhanes.clean %&gt;%\n  drop_na(AUQ300) %&gt;%\n  ggplot(aes(AUQ300, fill=AUQ300)) +\n  geom_bar(aes(AUQ300)) +\n  labs(x = \"Gun use\", y = \"Number of participants\", \n       subtitle = \"Filled inside the aes()\") \n\n\n\n\n\n\n\n\n\n\n\n5.1.2.5 Data Prep and Then Visualized\n\nIn the command below, we create a gss.2016.cleaned object to make a barplot. In doing so, we do the following:\nCreate a bar graph using the ggplot() command, which requires an aes() quoting function. This function says that we want to use the grass variable in our bar graph.\nDrop all NAs from the grass variable so that legal and not legal are the only categories.\nWe then create the bars and fill them with 2 colors, red and purple. Many color codes can be used here, and will be discussed in a later lesson.\nWe then add labels to our graph on both x and y axis.\nFinally, we print the new graph, which is saved under the legalize.bar object.\nBelow, I brought back over the code from the last part in Data Preparation. You should still have this in your Chapter1.R file. We are going to use that file to create a graphics in R.\n\n\ngss.2016 &lt;- read_csv(file = \"data/gss2016.csv\")\ngss.2016.cleaned &lt;- gss.2016 %&gt;%\n    mutate(grass = as.factor(grass)) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"DK\")) %&gt;%\n    mutate(grass = na_if(x = grass, y = \"IAP\")) %&gt;%\n    mutate(grass = droplevels(x = grass)) %&gt;%\n    mutate(age = recode(age, `89 OR OLDER` = \"89\")) %&gt;%\n    mutate(age = as.numeric(x = age)) %&gt;%\n    mutate(age.cat = cut(x = age, breaks = c(-Inf, 29, 59, 74, Inf), labels = c(\"&lt; 30\",\n        \"30 - 59\", \"60 - 74\", \"75+\")))\n\n\nOnce the data is prepped, we can graph the variable or variables.\n\n\n# Make a Bar Graph for Grass Variable\nlegalize.bar &lt;- gss.2016.cleaned %&gt;%\n    drop_na(grass) %&gt;%\n    ggplot(aes(x = grass)) + geom_bar(fill = c(\"red\", \"purple\")) + labs(x = \"Should marijuana be legal?\",\n    y = \"Percent of responses\")\nlegalize.bar\n\n\n\n\n\n\n\n\n\n\n5.1.2.6 Edit The Graphic\n\nNext, we can edit these commands to include the age variable. The aes() quoting function has expanded to have the bars filled color using the grass variable, the age category has replaced the grass variable on the x axis, and there is a new a formula on the y axis to sum and count.\nWe also gave this a theme and updated the labels.\n\n\nlegalize.bar &lt;- gss.2016.cleaned %&gt;%\n    drop_na() %&gt;%\n    ggplot(aes(age.cat, y = 100 * (after_stat(count))/sum(after_stat(count)),\n        fill = grass)) + geom_bar(position = \"dodge\") + theme_minimal() +\n    labs(x = \"Age Category\", y = \"Percent of responses\")\n\nlegalize.bar\n\n\n\n\n\n\n\n\n\nEvaluate these 2 graphs and see what information you can get from them.\n\n\n\n\n5.1.3 Pie Chart\n\nA pie chart is a segmented circle whose segments portray the relative frequencies of the categories of a qualitative variable.\nSlices of pie in different colors represent the parts.\nIn this example, the firearm is divided by type to show parts of a whole, where the total of the proportions must add to 1.0 and the total of the percentages must add to 100%.\n\n\n# Importing data from working directory\nfbi.deaths &lt;- read.csv(\"data/fbi_deaths.csv\", stringsAsFactors = TRUE)\n# Selecting rows of interest for pie chart\nfbi.deaths.small &lt;- fbi.deaths[c(3, 4, 5, 6, 7), ]\nfbi.deaths.small &lt;- fbi.deaths.small %&gt;%\n    rename(Weapon = X)\n# Checking summary of fbi deaths\nsummary(fbi.deaths.small)\n\n                       Weapon      X2012          X2013          X2014     \n Firearms, type not stated:1   Min.   : 116   Min.   : 123   Min.   :  93  \n Handguns                 :1   1st Qu.: 298   1st Qu.: 285   1st Qu.: 258  \n Other guns               :1   Median : 310   Median : 308   Median : 264  \n Rifles                   :1   Mean   :1779   Mean   :1691   Mean   :1662  \n Shotguns                 :1   3rd Qu.:1769   3rd Qu.:1956   3rd Qu.:2024  \n Asphyxiation             :0   Max.   :6404   Max.   :5782   Max.   :5673  \n (Other)                  :0                                               \n     X2015          X2016     \n Min.   : 177   Min.   : 186  \n 1st Qu.: 258   1st Qu.: 262  \n Median : 272   Median : 374  \n Mean   :1956   Mean   :2201  \n 3rd Qu.:2502   3rd Qu.:3077  \n Max.   :6569   Max.   :7105  \n                              \n\n\n\nAgain, ggplot works in layers, so in order to make a pie, you need a few layers and have a few optional ones.\n\nThe aes() command specifies the variable to create the pie, in this case x2016.\ngeom_col() sets the borders of the pie and makes it visible.\ncoord_polar() command makes the pie circular.\ntheme_void() command is optional and adjusts the theme of the pie. to remove axis, background, etc.\n\n\n\nggplot(fbi.deaths.small, aes(x=\"\", y=X2016, fill=Weapon)) +\n  geom_col() + \n  coord_polar(\"y\", start=0) + \n  theme_void() \n\n\n\n\n\n\n\n\n\nFrom the pie, we can see that the majority of weapons that caused fbi gun related deaths are handguns followed by a type of firearm that is not stated.\n\n\n\n5.1.4 Comparison of Charts\n\nRecommended graphs for single categorical or factor type variable:\n\nBar graph, for showing relative group sizes.\nPie charts are available in R but are not recommended because they tend to be less clear for comparing group sizes.\n\nPie charts are difficult to read since the relative size of pie pieces is often hard to determine.\nPie charts take up a lot of space to convey little information.\nPeople often use fancy formatting like 3D, which takes up more space and makes understanding relative size of pie pieces even more difficult.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#graphs-for-a-single-continuous-variable",
    "href": "dataviz.html#graphs-for-a-single-continuous-variable",
    "title": "5  Data Visualization",
    "section": "5.2 Graphs for a Single Continuous Variable",
    "text": "5.2 Graphs for a Single Continuous Variable\n\nA continuous variable refers to a variable that can take any value over a range of values.\nA continuous variable needs to be numeric, and could be integer type or numeric type in R. Just like with graphs that include categorical variables, it is beneficial to do any data cleaning and investigation into the variable(s) before you begin. With continuous variables, this may require recoding the variable to coerce it to the appropriate data type and/or renaming it to something meaningful if needed.\nIt is also beneficial to make sure the numerical variable is indeed supposed to be numerical (as opposed to a factor). For instance, you commonly see numbers listed for categories like the Yes/No coded as a 1/2, such as with the AUQ300 variable.\n\nSome common graphing options for single continuous variable:\n\nHistograms (From Lesson 2)\nDensity plots\nBoxplots\nViolin plots\n\n\n5.2.1 Histograms\n\nA histogram is a useful plot to determine central tendency and spread.\nWe went over histograms in Lesson 2, so refer back for information on how to create a histogram using base R and ggplot.\nRemember that you can tell the distribution from a histogram, and that distribution can be normal or skewed (Right or Left).\n\nWith each chart based on quantitative data, you should be able to get a sense of the distribution.\nThe histogram below looks right skewed.\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.2 Density Plots\n\nA density plot is similar to a histogram but more fluid in appearance because it does not have the separate bins.\n\nProbability density is not very useful for interpreting what is happening at any given value of the variable on the x-axis, but it is useful in computing the percentage of values that are within a range along the x-axis.\nThe area under the curve in a density plot could be interpreted as the probability of a single observation or a range of observations.\nWe can use random normal data to create the density plot like shown below with a sample of 1000, a mean of 10 and a standard deviation of 2. To do this, we need to make the vector and assign it to a data frame.\nIn R, set.seed() is a function used to set the seed for random number generation. By setting a seed using set.seed(), you ensure reproducibility of your code. If you run the same code with the same seed, you’ll get the same sequence of random numbers every time. This is particularly useful for debugging, testing, or when you want to ensure that your results are reproducible.\nWe use set.seed before any function with a random normal generator to ensure reproducibility.\nIf a dataset is provided, then you do not need to generate your own random data as shown in the step below.\n\n\n\nset.seed(1)\nx &lt;- rnorm(1000, mean = 10, sd = 2)\ndf &lt;- data.frame(x)\n\n\nNext, we can make the density plot using the ggplot2 package under tidyverse.\n\nLayer 2 includes the geom_density() command in addition to the standard Layer 1 ggplot() command to create the density plot.\n\n\n\nggplot(df, aes(x)) + geom_density()\n\n\n\n\n\n\n\n\n\nThere are a lot of arguments you can change. I selected a couple below. Be sure to look at the help file on the geom_density() layer to get the variety on what you can do.\n\ncolor = sets a line color\nlwd = makes the line thicker. Increase this number for thicker line.\nfill= colors the area under the curve.\nalpha= sets the transparency to the area under the curve.\n\n\nggplot(df, aes(x)) + geom_density(color = \"darkblue\", lwd = 3)\n\n\n\n\n\n\n\n\n\n\nggplot(df, aes(x)) +\n  geom_density(color = \"darkblue\", \n    #fill = Fills color under the curve\n    fill = \"lightblue\",\n    #alpha = Sets a transparency to the area under the curve. \n    #.5 for 50% transparency. \n    alpha = .5) \n\n\n\n\n\n\n\n\n\nWe can even add a mean line, which we know in this case is 10 because we used random normal data with that mean set as a parameter.\ngeom_vline() is a function used to add vertical lines to a plot created with ggplot. This function is useful for visually indicating specific points or ranges on the x-axis.\nYou can do a line break in your R code after a comma (\\(,\\)) or after a plus sign (\\(+\\)). I find things easier to read on less lines, but it is personal preference how many lines you use given still following the rules in R.\n\n\n\nggplot(df, aes(x)) + geom_density(color = \"darkblue\", fill = \"lightblue\",\n    alpha = 0.5) + geom_vline(aes(xintercept = mean(x)), color = \"red\",\n    linetype = \"dashed\", lwd = 1)\n\n\n\n\n\n\n\n\n\n\n5.2.3 Boxplot\n\nA boxplot is a visual representation of data that shows central tendency (usually the median) and spread (usually the interquartile range) of a numeric variable for one or more groups.\nBoxplots are often used to compare the distribution of a continuous variable across several groups.\nA box plot allows you to:\n\nGraphically display the distribution of a data set.\nCompare two or more distributions.\nIdentify outliers in a data set.\n\n\n\n\n\nA Boxplot with Outliers on Left\n\n\n\nBoxplots include the following information:\n\nA line representing the median value.\nA box containing the middle 50% of values.\nWhiskers extending to 1.5 times the IQR.\nOutliers more than 1.5 times the IQR away from the median.\n\n\n\n\n\nA Boxplot with No Outliers\n\n\n\nThis boxplot above displays 5 summary values:\n\nS = smallest value.\nL = largest value.\nQ1 = first quantile = 25th percentile.\nQ2 = median = second quantile = 50th percentile.\nQ3 = third quantile = 75th percentile.\n\nFor example, use the GrowthFund Vector from the last lesson. It is executed again below.\n\n\nGrowthFund &lt;- c(-38.32, 1.71, 3.17, 5.99, 12.56, 13.47, 16.89, 16.96, 32.16,\n    36.29)\nGrowthFund &lt;- as.data.frame(GrowthFund)\n\n\nThe quantile() function returns the five point summary when no arguments are specified.\n\n\nQuanData &lt;- quantile(GrowthFund$GrowthFund)\nQuanData\n\n      0%      25%      50%      75%     100% \n-38.3200   3.8750  13.0150  16.9425  36.2900 \n\n\n\n\n5.2.4 Detecting Outliers\n\nWe see an outlier visually, but without the tool available, we can detect them through the statistics. First we calculate the IQR, which is just quarter 3 minus quarter 1.\n\n\nsummary(GrowthFund$GrowthFund)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-38.320   3.875  13.015  10.088  16.942  36.290 \n\nIQRvalue &lt;- 16.9425 - 3.875\nIQRvalue\n\n[1] 13.0675\n\nIQRvalue &lt;- IQR(GrowthFund$GrowthFund)\nIQRvalue\n\n[1] 13.0675\n\n\n\nThen, multiply the IQR by 1.5.\n\n\nOutlierValue &lt;- IQRvalue * 1.5\nOutlierValue\n\n[1] 19.60125\n\n\n\nFinally conduct 2 checks to determine if outliers are past the low whisker and/or high whisker.\n\nA TRUE value indicates that at least one outlier is present at the small end of the distribution.\nA FALSE value indicates that no outliers are at the high end of the distribution.\n\n\nQuanData\n\n      0%      25%      50%      75%     100% \n-38.3200   3.8750  13.0150  16.9425  36.2900 \n\nQuanData[2] - QuanData[1] &gt; OutlierValue\n\n 25% \nTRUE \n\n# True indicating an outlier to the left\n3.875 - -38.32  #42.195\n\n[1] 42.195\n\n42.195 &gt; 19.60125  #TRUE\n\n[1] TRUE\n\nQuanData[4] - QuanData[5] &gt; OutlierValue\n\n  75% \nFALSE \n\n# False indicating no outlier to the right.\n16.9425 - 36.29  #-19.3475\n\n[1] -19.3475\n\n-19.3475 &gt; 19.60125  #FALSE \n\n[1] FALSE\n\n\n\n\n5.2.4.1 GrowthFund Boxplot\n\nWe can use ggplot to retrieve our graph and associated numbers.\nThe outlier is visually depicted on the graph as -38.32.\n\n\nggplot(GrowthFund, aes(GrowthFund)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe can add a little color to the plot with the fill parameter, but then we also need to be sure to turn off the legends in the geom_boxplot.\n\n\nggplot(GrowthFund, aes(GrowthFund, fill = \"red\")) + geom_boxplot(show.legend = FALSE)\n\n\n\n\n\n\n\n\n\nYou can add parameters to make this visualization more professional, but this gets you started. Be sure to look at some examples in the R community or on ChatGPT.\n\n\n\n\n5.2.5 Violin Plots\n\nA visual display of data that combines features of density plots and boxplots to show the distribution of numeric variables, often across groups.\n\n\n5.2.5.1 GrowthFund Example\n\nWe can look at the GrowthFund example we had as a boxplot above as a violin plot. In order to make the change, we alter the second layer from geom_boxplot() to geom_violin().\n\n\nGrowthFund %&gt;% ggplot(aes(x=\"\", y=GrowthFund))+ \n  geom_violin() + theme_minimal() + coord_flip()\n\n\n\n\n\n\n\n\n\n\n5.2.5.2 mpg Example\n\nWe can also view mpg from the mtcars data set as a violin plot because it is a numerical variable. In the plot below, I graphed mpg as a violin plot. You can also embed other visual markers like mean and median or layer on another graph like a boxplot.\nYou can see that this violin plot is vertical. The coord_flip() command we used above flips the chart horizontal the same way it flips a boxplot.\n\n\nmtcars %&gt;%\n  ggplot(aes(x=\"\", y = mpg)) +\n  geom_violin(fill=\"lightgreen\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\nWe could also merge both a violin and a boxplot. The code below shows two separate charts (and also flipped) using the following code and then merges them into one visualization.\nThe width parameter controls the width of the boxes in a boxplot or the width of the violins in a violin plot. You can vary this parameter to create better depth.\n\n\nlibrary(tidyverse)\ndata(mtcars)\n\n\nmtcars %&gt;%\n    ggplot(aes(x = \"\", y = mpg)) + geom_boxplot(color = \"#986D00\", fill = \"#24585E\") +\n    theme_minimal() + coord_flip()\n\n\n\n\n\n\n\nmtcars %&gt;%\n    ggplot(aes(x = \"\", y = mpg)) + geom_violin(color = \"#986D00\", fill = \"#24585E\") +\n    theme_minimal() + coord_flip()\n\n\n\n\n\n\n\nmtcars %&gt;%\n    ggplot(aes(x = \"\", y = mpg)) + geom_violin(fill = \"lightgreen\") + theme_minimal() +\n    geom_boxplot(width = 0.1, color = \"#986D00\", fill = \"#24585E\")\n\n\n\n\n\n\n\n\n\nLet’s get some summary statistics to check for outliers, skewness, and kurtosis to see how our visual aids help us in understanding those results.\n\n\nIQRvalue &lt;- IQR(mtcars$mpg)\nOutlierValue &lt;- IQRvalue * 1.5\nOutlierValue\n\n[1] 11.0625\n\nQuanData &lt;- quantile(mtcars$mpg)\nQuanData\n\n    0%    25%    50%    75%   100% \n10.400 15.425 19.200 22.800 33.900 \n\nQuanData[2] - QuanData[1] &gt; OutlierValue\n\n  25% \nFALSE \n\n# False indicating no outlier to the left\nQuanData[4] - QuanData[5] &gt; OutlierValue\n\n  75% \nFALSE \n\n# False indicating no outlier to the right.\nsemTools::skew(mtcars$mpg)\n\nskew (g1)        se         z         p \n0.6723771 0.4330127 1.5527885 0.1204737 \n\nsemTools::kurtosis(mtcars$mpg)\n\nExcess Kur (g2)              se               z               p \n    -0.02200629      0.86602540     -0.02541068      0.97972740 \n\n\n\nLooks like the mpg variable is quite normal with no signs of outliers, skewness, or kurtosis. In the boxplot, it looks like there might be an outlier to the right, but it is not problematic to the dataset as indicated by the manual calculations.\nWe can also get a histogram of mpg and are able to make the same claims towards normality. You can see a slight pull to the right, but it is seemingly normal.\n\n\nggplot(mtcars, aes(mpg)) + geom_histogram(binwidth = 5, color = \"black\",\n    fill = \"green\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#graphs-for-two-variables-at-once",
    "href": "dataviz.html#graphs-for-two-variables-at-once",
    "title": "5  Data Visualization",
    "section": "5.3 Graphs for Two Variables At Once",
    "text": "5.3 Graphs for Two Variables At Once\n\nCombinations of 2 Variable Types for Graphing\n\nTwo categorical/ factor.\nOne categorical/ factor and one continuous/ numeric.\nTwo continuous/ numeric.\n\n\n\n5.3.1 Bar Graphs for Two Categorical Variables\n\nThere are two formats available for bar charts:\n\nGrouped\nStacked\n\n\n\n\n# A tibble: 6 × 3\n# Groups:   vs, gear [6]\n     vs  gear     n\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     0     3    12\n2     0     4     2\n3     0     5     4\n4     1     3     3\n5     1     4    10\n6     1     5     1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.1.1 Grouped Bar Graph\n\nGrouped bar graph allow comparison of multiple sets of data items, with a single color used to denote a specific series across all sets.\nFor example, we can look at both the vs and gear variables in the ggplot command.\n\nWe did a little grouping and counting before we began to generate a new table with frequencies based on vs and gear variables.\nOnce the dataset was made, we can make the graph with the geom_bar layer.\nSince there are two variables, we need to set the position to dodge.\n(stat = “identity”) tells ggplot that the y values are already computed and should be used as-is for the heights of the bars. In this case, they are frequencies calculated in the countsDF dataset.\n\n\nmtcars &lt;- mtcars %&gt;%\n    mutate(vs = as.factor(vs)) %&gt;%\n    mutate(gear = as.factor(gear))\n\n\ncountsDF &lt;- mtcars %&gt;%\n    group_by(vs, gear) %&gt;%\n    count()\n\nsummary(countsDF)\n\n vs    gear        n         \n 0:3   3:2   Min.   : 1.000  \n 1:3   4:2   1st Qu.: 2.250  \n       5:2   Median : 3.500  \n             Mean   : 5.333  \n             3rd Qu.: 8.500  \n             Max.   :12.000  \n\nggplot(countsDF, aes(x = gear, y = n, fill = vs)) + geom_bar(stat = \"identity\",\n    position = \"dodge\") + labs(title = \"Grouped Car Distribution by Gears and VS\",\n    x = \"Number of Gears\", y = \"Count\") + theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n5.3.1.2 Stacked Bar Graph\n\nA Stacked bar graph extends the standard bar graph from looking at numeric values across one categorical variable to two. Each bar in a standard bar graph is divided into a number of sub-bars stacked end to end, each one corresponding to a level of the second categorical variable.\nUsing ggplot, we can also stack these charts by removing the position = dodge statement.\n\n\nggplot(countsDF, aes(x = gear, y = n, fill = vs)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Stacked Car Distribution\",\n       x = \"Number of Gears\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n5.3.1.3 Bar Graph for Continuous Across Groups\n\nIn comparison to a bar graph for a single categorical variable, a bar chart for a continuous variable across groups includes both a x and y axis. The continuous variable is put on the y axis, and the categorical (factor) variable is placed on the x axis showing the groups.\nTherefore, instead of counting data based on group, we can see another continuous variable based on group data.\nThe frequency data (i.e., counts) can be replaced with another numerical variable like mean.\nIn the below example, instead of counting observations per group, here, we took the average mpg (a continuous variable) based on groups of gear and vs and summarized the data into a variable avg_mpg. We then used that variable in a ggplot() command to create a unique chart to that above.\n\n\navg_mpg &lt;- mtcars %&gt;%\n    group_by(gear, vs) %&gt;%\n    summarise(mpg = mean(mpg, na.rm = TRUE))\n\n\nggplot(avg_mpg, aes(gear,\n  mpg, fill = vs)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  ggtitle(\"Average MPG by VS and Gear\")\n\n\n\n\n\n\n\n\n\nWe can add color using the scale_fill_manual. Since there are two colors, we use the c() command to combine them together inside the layer.\n\nscale_fill_manual() is a function used to manually set the colors for filled areas (like bars, areas in geom_area, etc.) in a plot. Here, values argument takes our custom color palette of yellow and brown.\n\n\nggplot(avg_mpg, aes(gear, mpg, fill = vs)) + geom_bar(stat = \"identity\",\n    position = \"dodge\", color = \"black\") + ggtitle(\"Average MPG by VS and Gear\") +\n    scale_fill_manual(values = c(\"yellow\", \"brown\"))\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Boxplot for Continuous Across Groups\n\nA boxplot requires one continuous variable (like we did above). When we include an additional grouping variable, we get multiple boxplots, one for each group. This allows us to directly compare distributions.\nThe categorical variable should correctly be a factor data type before you begin.\nIn the example below, mpg is the continuous variable, and gear is the categorical variable.\nWe see 3 boxplots for three values of gear (3, 4, 5). ggplot() and geom_boxplot() are required components of the command. The scale_fill_manual() and theme_minimal() layers are optional ways to change the style and color.\n\n\nmtcars %&gt;%\n  ggplot(aes(x = gear, y = mpg)) +\n  geom_boxplot(aes(fill = gear)) +\n  scale_fill_manual(values = c(\"gray\", \"red\", \"blue\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLets alter the functions above to depict mpg based on vs with categorical states 0 and 1.\n\n\nmtcars %&gt;%\n    ggplot(aes(x = vs, y = mpg)) + geom_boxplot(aes(fill = vs), show.legend = FALSE) +\n    scale_fill_manual(values = c(\"gray\", \"red\")) + theme_minimal()\n\n\n\n\n\n\n\n\n\n\n5.3.3 Scatterplot for Two Continuous Variables\n\n5.3.3.1 Scatterplots\n\nA scatterplot is used to determine if two continuous variables are related.\n\nEach point is a pairing: \\((x_1, y_1),(x_2, y_2),\\) etc.\n\nOur goal with a scatterplot is to characterize the relationship by visual inspection. This includes determining if the relationship looks positive, negative, or not existent.\n\n\n\n\nScatterPlot Results\n\n\n\nSometimes, it is really clear how to characterize the relationship. Other times, additional statistical tests are needed to confirm the relationship (which we will go over in later lessons). This is true especially with big data, where the plot window can look like a giant blog of observations.\nLet’s work a clean example examining the relationship between income and the years of education one has had.\nThis plot has a clear positive trend, meaning that as one has more years of education, we see higher income. And similarly, as we see higher income, we also see more years of education. This means that a scatter can help characterize the relationship, and does not state that one variable is causing another to occur.\n\n\nEdu &lt;- read.csv(\"data/education.csv\")\nplot(Edu$Income ~ Edu$Education, ylab = \"Income\", xlab = \"Education\")\n\n\n\n\n\n\n\n\n\nWorking with ggplot instead of base R, we would use the following code.\n\nLayer 1: ggplot() command with aes() command directly inside of it pointing to x and y variables.\nLayer 2: geom_point() command to add the observations as indicators in the chart.\nLayer 3 or more: many other optional additions like labs (for labels) as shown below.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point() + labs(y = \"Income\",\n    x = \"Education\")\n\n\n\n\n\n\n\n\ngeom_point() has some parameters you can change like shape = where you can add depth to your chart. Some common shapes of geom_point are as follows.\n\n\n# shape = 0, square shape = 1, circle shape = 2, triangle point up\n# shape = 3, plus shape = 4, cross shape = 5, diamond shape = 6,\n# triangle point down shape = 7, square cross shape = 8, star shape =\n# 9, diamond plus shape = 10, circle plus shape = 11, triangles up\n# and down shape = 12, square plus shape = 13, circle cross shape =\n# 14, square and triangle down shape = 15, filled square shape = 16,\n# filled circle shape = 17, filled triangle point-up shape = 18,\n# filled diamond\n\n\nFor instance, the code below changes the color, shape, and size of the geom_point().\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point(color = \"#183028\",\n    shape = 18, size = 10) + labs(y = \"Income\", x = \"Education\")\n\n\n\n\n\n\n\n\n\nggplot allows us to add a geom_line, which is helpful in drawing a line through the data. Here, I am also resetting the color off of the default value. You see this a lot on time series models like stock charts.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point(color = \"#183028\",\n    shape = 18, size = 4) + labs(y = \"Income\", x = \"Education\") + geom_line(color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nAnother way to do this is to add a stat_smooth line, which is considered a trendline to help us visualize the relationship between the variables.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point(color = \"#183028\",\n    shape = 18, size = 4) + labs(y = \"Income\", x = \"Education\") + stat_smooth(color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nWe can change the type of trendline. The most common is the to develop the trendline using the lm method, which stands for the linear method that we are going to learn in the Regression lesson. For now, lets insert _method=“lm” into our stat_smooth later to see the change.\n\n\nggplot(Edu, aes(x = Education, y = Income)) + geom_point() + labs(y = \"Income\",\n    x = \"Education\") + stat_smooth(method = \"lm\", color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nLet’s look at a few more examples and see if the relationship is considered positive, negative, or not existent.\nBelow, we see a negative trend.\n\n\nggplot(mtcars, aes(x = disp, y = mpg)) + geom_point() + stat_smooth(method = \"lm\",\n    color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nIn regards to the hp variable, below, we see another negative trend.\n\n\nggplot(mtcars, aes(x = hp, y = mpg)) + geom_point() + stat_smooth(method = \"lm\",\n    color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nIn regards to the qsec variable, below, we see a weak positive trend. This relationship would need to verified later on.\n\n\nggplot(mtcars, aes(x = qsec, y = mpg)) + geom_point() + stat_smooth(method = \"lm\",\n    color = \"#789F90\")\n\n\n\n\n\n\n\n\n\nThe plot() command also works when you do not have 2 continuous variables, and instead have one categorical variable paired with one continuous variable. However, the plot is not as adequate as others are in inferring the relationship from the variables.\nFor example, the plot below would be better served as a boxplot.\n\n\nplot(mtcars$mpg ~ mtcars$vs)\nboxplot(mtcars$mpg ~ mtcars$vs)\n\n\n\n\n\n\n\n\n\nUsing ggplot, we would have the same issue. Since vs is a categorical variable, it does not look right when we use the geom_point() later.\n\n\nggplot(mtcars, aes(x = vs, y = mpg)) + geom_point() + stat_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nInstead, we would want the geom_boxplot() layer like shown below. The stat_smooth() is also not an applicable layer to a boxplot and would need to be removed.\n\n\n\nggplot(mtcars, aes(x = vs, y = mpg)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n5.3.3.2 Try to Recreate\n\nYou try examples of scatterplots using the UScrime data set that is part of the MASS package to examine a few relationships using ggplot2. A few examples are below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee what information you can take away from the scatterplots above and create some more to practice.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#full-example-using-nhanes-dataset",
    "href": "dataviz.html#full-example-using-nhanes-dataset",
    "title": "5  Data Visualization",
    "section": "5.4 Full Example Using nhanes Dataset",
    "text": "5.4 Full Example Using nhanes Dataset\n\nA big example of where charts are helpful is to explore the nhanes dataset, which is actually a dataset related to auditory issues, in which the number of guns fired is a variable.\nSpecifically, AUQ060 refers to “Hear a whisper from across a quiet room?” AUQ070 refers to “Hear normal voice across a quiet room?” and AUQ080 refers to “Hear a shout from across a quiet room?” While AUQ300 refers to “Ever used firearms for any reason? AUQ310 refers to”How many total rounds ever fired?” and AUQ320 refers to “Wear hearing protection when shooting?” I got these references at the following website: https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/AUQ_G.htm.\nIt would be interesting to determine whether there is a relationship between hearing loss and gun use. Below, I clean the data set and make some charts. All variables are categorical, and I only look at 2 categorical variables at a time - one auditory related (AUQ 060, 070, 080) and one gun related (AUQ 300, 310, 320).\nThe first step is data cleaning, and in particular recoding the variables of interest. We did some of these before. Let’s also filter out the unused variables using the select statement.\n\nThe select statement in dplyr has a conflict with the select statement in MASS. Since we used MASS earlier, we have to specify which package we want to use select from. We want to use select from dplyr, so we add dplyr:: before the function.\n\n\nnhanes &lt;- read.csv(\"data/nhanes2012.csv\")\n# summary(nhanes)\n\nnhanes.clean &lt;- nhanes %&gt;%\n    dplyr::select(AUQ300, AUQ310, AUQ320, AUQ060, AUQ070, AUQ080) %&gt;%\n    mutate(AUQ300 = recode_factor(AUQ300, `1` = \"Yes\", `2` = \"No\")) %&gt;%\n    mutate(AUQ310 = recode_factor(AUQ310, `1` = \"1 to less than 100\", `2` = \"100 to less than 1000\",\n        `3` = \"1000 to less than 10k\", `4` = \"10k to less than 50k\", `5` = \"50k or more\",\n        `7` = \"Refused\", `9` = \"Don't know\")) %&gt;%\n    mutate(AUQ060 = recode_factor(AUQ060, `1` = \"Yes\", `2` = \"No\")) %&gt;%\n    mutate(AUQ070 = recode_factor(AUQ070, `1` = \"Yes\", `2` = \"No\")) %&gt;%\n    mutate(AUQ080 = recode_factor(AUQ080, `1` = \"Yes\", `2` = \"No\")) %&gt;%\n    mutate(AUQ320 = recode_factor(AUQ320, `1` = \"Always\", `2` = \"Usually\",\n        `3` = \"About half the time\", `4` = \"Seldom\", `5` = \"Never\"))\nsummary(nhanes.clean)\n\n  AUQ300                       AUQ310                     AUQ320    \n Yes :1613   1 to less than 100   : 701   Always             : 583  \n No  :3061   100 to less than 1000: 423   Usually            : 152  \n NA's:4690   1000 to less than 10k: 291   About half the time: 123  \n             10k to less than 50k : 106   Seldom             : 110  \n             50k or more          :  66   Never              : 642  \n             Don't know           :  26   NA's               :7754  \n             NA's                 :7751                             \n  AUQ060      AUQ070      AUQ080    \n Yes :2128   Yes : 564   Yes : 159  \n No  : 745   No  : 210   No  :  53  \n NA's:6491   NA's:8590   NA's:9152  \n                                    \n                                    \n                                    \n                                    \n\n\n\nFrom here, NA’s are an issue, but we don’t want to broadly omit because it would slice down our dataset too much. I am going to leave them in and handle it on a chart by chart basis.\nThe most applicable chart to graph 2 categorical variables is a barchart. This requires calculating frequencies, and then graphing. Using ggplot, the frequencies are calculated automatically.\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ310) %&gt;%\n    drop_na(AUQ060) %&gt;%\n    ggplot(aes(x = AUQ310, fill = AUQ060)) + geom_bar(position = \"dodge\") +\n    labs(x = \"How many rounds have you fired\", title = \"Hearing Whisper Across Room vs. Num Rounds Fired\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ310) %&gt;%\n    drop_na(AUQ070) %&gt;%\n    ggplot(aes(x = AUQ310, fill = AUQ070)) + geom_bar(position = \"dodge\") +\n    labs(x = \"How many rounds have you fired\", title = \"Hearing Normal Across Room vs. Num Rounds Fired\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ310) %&gt;%\n    drop_na(AUQ080) %&gt;%\n    ggplot(aes(x = AUQ310, fill = AUQ080)) + geom_bar(position = \"dodge\") +\n    labs(x = \"How many rounds have you fired\", title = \"Hearing Shout Across Room vs. Num Rounds Fired\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\n############# Wear hearing protection when shooting\nnhanes.clean %&gt;%\n    drop_na(AUQ320) %&gt;%\n    drop_na(AUQ060) %&gt;%\n    ggplot(aes(x = AUQ320, fill = AUQ060)) + geom_bar(position = \"dodge\") +\n    labs(x = \"Wear hearing protection when shooting\", title = \"Hearing Whisper Across Room vs. Use of Hearing Protection\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ320) %&gt;%\n    drop_na(AUQ070) %&gt;%\n    ggplot(aes(x = AUQ320, fill = AUQ070)) + geom_bar(position = \"dodge\") +\n    labs(x = \"Wear hearing protection when shooting\", title = \"Hearing Normal Across Room vs. Use of Hearing Protection\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\nnhanes.clean %&gt;%\n    drop_na(AUQ310) %&gt;%\n    drop_na(AUQ080) %&gt;%\n    ggplot(aes(x = AUQ310, fill = AUQ080)) + geom_bar(position = \"dodge\") +\n    labs(x = \"Wear hearing protection when shooting\", title = \"Hearing Shout Across Room vs. Use of Hearing Protection\",\n        y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nThat is a few of the charts. See if you can determine anything interesting from them and also try to run the other combinations. You will see that it is easier to see with less categories when you have multiple variables like this.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#summary",
    "href": "dataviz.html#summary",
    "title": "5  Data Visualization",
    "section": "5.5 Summary",
    "text": "5.5 Summary\n\nPractice many more examples with the help of ChatGPT and work towards constructing high-quality charts and graphs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In this online book, we went over information to make sure you had some basics to really start learning R. This journey began with the basics of R, emphasizing that while multiple methods exist to achieve the same results in R, it’s crucial to find the approach that works best for you. As we learn R, you will get used to doing things your way to be able to slice and evaluate the data to find rich information from the data sets we look at. As long as the data was handled properly, it does not matter how we reach our goal using R as long as we do it ourselves.\nMastering R allows you to effectively clean, analyze, and interpret data, unlocking valuable insights from various datasets. We explored the essential practice of data cleaning, learning various techniques and popular functions within the dplyr package under the tidyverse. Proper data cleaning ensures the integrity and accuracy of your analysis. We delved into skewness, kurtosis, variables, and scales of measurement, focusing on summarizing qualitative and quantitative data. Visualizations were introduced as powerful tools to describe variables and uncover patterns in the data.\nThis book has equipped you with the foundational skills needed to handle data before deeply analyzing it using R. Remember, the key to mastering these techniques lies in consistent practice and finding the methods that best suit your analytical style.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "7  Recommended Further Reading Materials",
    "section": "",
    "text": "Harris, J. K. (2019). Statistics with R: Solving problems using real-world data. SAGE Publications.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning: With applications in R. Springer. (Springer Texts in Statistics)\nJaggia, S., & Kelly, A. (2018). Business statistics: Communicating with numbers (3rd ed.). McGraw-Hill Education. }",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Recommended Further Reading Materials</span>"
    ]
  }
]